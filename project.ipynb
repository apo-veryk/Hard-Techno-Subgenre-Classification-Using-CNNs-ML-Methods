{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCrTj-er5Y7m",
        "outputId": "a8a9ab0e-4687-4cc1-efa2-775837eb8571"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/MyDrive/MSc/deep_learning\"\n",
        "# !dir \"data/melgrams\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JGr9Z7o9nTFg"
      },
      "outputs": [],
      "source": [
        "#setting input files and folder-dirs\n",
        "\n",
        "\n",
        "\n",
        "CLASSES = ['bouncy','tekno','warzone','industrial','non-techno-drop']\n",
        "# REMOVED: 'psytech',\n",
        "\n",
        "# csv_file_path = \"/content/drive/MyDrive/MSc/deep_learning/labels_final.csv\"\n",
        "# tracks_dir = \"/content/drive/MyDrive/MSc/deep_learning/tracks_final\"\n",
        "# clips_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/clips\"\n",
        "\n",
        "# norm_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/norm_melgrams_dir\"\n",
        "# low_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/low_melgrams\"\n",
        "# melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "\n",
        "csv_file_path = \"data/labels_final.csv\"\n",
        "tracks_dir = \"data/tracks_final\"\n",
        "clips_dir = \"data/clips\"\n",
        "\n",
        "norm_melgrams_dir = \"data/melgrams\"\n",
        "low_melgrams_dir = \"data/low_melgrams\"\n",
        "melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "import os\n",
        "os.makedirs(clips_dir, exist_ok=True)\n",
        "os.makedirs(norm_melgrams_dir, exist_ok=True)\n",
        "os.makedirs(low_melgrams_dir, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# import random, numpy as np\n",
        "# from tensorflow.keras.utils import set_random_seed\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# tf.random.set_seed(SEED)\n",
        "# set_random_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F6T-Mljy-TW9",
        "outputId": "57943e42-7689-4a78-98fb-817af7e979c2"
      },
      "outputs": [],
      "source": [
        "# # sample-clipper > clips are within \"deep_learning/clips\" (including non-techno-drop)\n",
        "\n",
        "\n",
        "\n",
        "# import os, pandas as pd, librosa, soundfile as sf\n",
        "\n",
        "# df = pd.read_csv(csv_file_path)\n",
        "# total_clips = len(df)\n",
        "# processed = 0\n",
        "\n",
        "# # convert m:ss to float seconds\n",
        "\n",
        "# def ts_to_sec(ts: str) -> float:\n",
        "#     if \":\" in ts:\n",
        "#         m, s = map(int, ts.split(\":\"))\n",
        "#         return m * 60 + s\n",
        "#     else:\n",
        "#         return float(ts)\n",
        "\n",
        "# # iterating rows and create clips\n",
        "\n",
        "# for idx, row in df.iterrows():\n",
        "#     spot_id    = str(row[\"spot_id\"])\n",
        "#     sample_id  = str(row[\"sample_id\"])\n",
        "#     clip_start = ts_to_sec(str(row[\"clip_start\"]))\n",
        "\n",
        "#     # skip if clip exists\n",
        "#     dst = os.path.join(clips_dir, f\"{sample_id}.mp3\")\n",
        "#     if os.path.exists(dst):\n",
        "#         print(f\"[SKIP] Clip exists: {sample_id}.mp3\")\n",
        "#         processed += 1\n",
        "#         continue\n",
        "\n",
        "#     duration = 8\n",
        "\n",
        "#     src = os.path.join(tracks_dir, f\"{spot_id}.mp3\")\n",
        "\n",
        "#     if not os.path.exists(src):\n",
        "#         print(f\"[SKIP] track file not found: {src}\")\n",
        "#         processed += 1\n",
        "#         continue\n",
        "\n",
        "#     try:\n",
        "#         y, sr = librosa.load(src, sr=None)\n",
        "#         beg = int(clip_start * sr)\n",
        "#         end = int((clip_start + duration) * sr)\n",
        "#         clip = y[beg:end]\n",
        "\n",
        "#         sf.write(dst, clip, sr)\n",
        "#         processed += 1\n",
        "#         print(f\"[OK] {sample_id}.mp3 | {duration}s ({processed}/{total_clips})\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"[ERR] {sample_id}: {e}\")\n",
        "\n",
        "# print(f\"done {processed}/{total_clips} clips.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiUZYUL3dCr9",
        "outputId": "93ad32ba-5550-4280-cffd-5f531c219a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1772\n"
          ]
        }
      ],
      "source": [
        "### see how many we clips we got after all\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "n_written = sum(1 for f in os.listdir(clips_dir))\n",
        "print(n_written)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hd3dBypA0Gh",
        "outputId": "59e73072-5ca9-4b98-b767-06ba4dbeb5a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n"
          ]
        }
      ],
      "source": [
        "### features for ML predictions with librosa\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# function to get features with librosa\n",
        "def extract_features(audio_file):\n",
        "    y, sr = librosa.load(audio_file)\n",
        "\n",
        "    # feature list\n",
        "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    rmse = librosa.feature.rms(y=y)\n",
        "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "\n",
        "    # getting means\n",
        "    return [\n",
        "        np.mean(chroma_stft),\n",
        "        np.mean(rmse),\n",
        "        np.mean(spec_cent),\n",
        "        np.mean(spec_bw),\n",
        "        np.mean(rolloff),\n",
        "        np.mean(zcr),\n",
        "        np.mean(mfcc[0]),\n",
        "        np.mean(mfcc[1]),\n",
        "        np.mean(mfcc[2]),\n",
        "        np.mean(mfcc[3]),\n",
        "        np.mean(mfcc[4]),\n",
        "        np.mean(mfcc[5]),\n",
        "        np.mean(mfcc[6]),\n",
        "\n",
        "    ]\n",
        "\n",
        "# going through the clips\n",
        "for filename in os.listdir(clips_dir):\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        clip_path = os.path.join(clips_dir, filename)\n",
        "\n",
        "        # getting features\n",
        "        clip_features = extract_features(clip_path)\n",
        "        features.append(clip_features)\n",
        "\n",
        "        # getting the name as well, to match them later\n",
        "        sample_id = filename[:-4]\n",
        "\n",
        "        # matching with the labels from the csv\n",
        "        try:\n",
        "            row = df_csv.loc[df_csv['sample_id'] == sample_id, CLASSES].iloc[0].tolist()\n",
        "            # convert duration â†’ binary flag (misc)\n",
        "            misc_val   = row[-1]\n",
        "            misc_label = 1 if pd.notna(misc_val) and int(misc_val) > 0 else 0\n",
        "            row[-1]    = misc_label\n",
        "\n",
        "        except IndexError:\n",
        "            print(f\"[WARN] no label for {sample_id}\")\n",
        "            row = [np.nan]*8\n",
        "\n",
        "        labels.append(row)\n",
        "\n",
        "\n",
        "# now we will get it all into a dataframe to work with it in the svm\n",
        "feature_names = [\n",
        "    'chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'\n",
        "]\n",
        "df_features = pd.DataFrame(features, columns=feature_names)\n",
        "df_labels = pd.DataFrame(labels, columns=CLASSES)\n",
        "df_final = pd.concat([df_features, df_labels], axis=1)\n",
        "\n",
        "# saving it\n",
        "df_final.to_csv(\"data/clip_features_and_labels_2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZK_aEIiU6KHu",
        "outputId": "2f10441e-d616-4904-855b-0cac64ce8d31"
      },
      "outputs": [],
      "source": [
        "# !pip install pyAudioAnalysis eyed3 pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ArIGzrTn5DkJ"
      },
      "outputs": [],
      "source": [
        "### features for ML predictions with pyAudioAnalysis\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from pyAudioAnalysis.audioBasicIO import read_audio_file\n",
        "from pyAudioAnalysis.ShortTermFeatures import feature_extraction\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# extract features with pyAudioAnalysis\n",
        "def extract_features(audio_file):\n",
        "    try:\n",
        "        [fs, x] = read_audio_file(audio_file)\n",
        "\n",
        "        # extracting features\n",
        "        [feats, _] = feature_extraction(\n",
        "            x, fs, 0.050 * fs, 0.025 * fs)\n",
        "\n",
        "        # feature mapping as before\n",
        "        return [\n",
        "            np.mean(feats[13:25]),\n",
        "            np.mean(feats[1]),\n",
        "            np.mean(feats[3]),\n",
        "            np.mean(feats[4]),\n",
        "            np.mean(feats[7]),\n",
        "            np.mean(feats[0]),\n",
        "            *[np.mean(feats[8+i]) for i in range(5)],\n",
        "            np.mean(feats[8:13]),\n",
        "            np.mean(feats[8:13])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "for filename in os.listdir(clips_dir):\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        clip_path = os.path.join(clips_dir, filename)\n",
        "\n",
        "        # get the features\n",
        "        try:\n",
        "            clip_features = extract_features(clip_path)\n",
        "            features.append(clip_features)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # match labels from csv\n",
        "        sample_id = filename[:-4]\n",
        "        try:\n",
        "            clip_labels = df_csv.loc[\n",
        "                df_csv['sample_id'] == sample_id, CLASSES].values.tolist()[0]\n",
        "        except IndexError:\n",
        "            print(f\"Warning: No label found for sample_id: {sample_id}\")\n",
        "            clip_labels = [np.nan] * 8\n",
        "\n",
        "        labels.append(clip_labels)\n",
        "\n",
        "# creating the dataframes\n",
        "feature_names = [\n",
        "    'chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'\n",
        "]\n",
        "df_features = pd.DataFrame(features, columns=feature_names)\n",
        "df_labels = pd.DataFrame(labels, columns=CLASSES)\n",
        "df_final = pd.concat([df_features, df_labels], axis=1)\n",
        "\n",
        "# saving\n",
        "df_final.to_csv(\"data/clip_features_and_labels_pyAudioAnalysis.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "1soqjxyp7ElX"
      },
      "outputs": [],
      "source": [
        "### filtering rows\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# loading the df from before\n",
        "df = pd.read_csv(\"data/clip_features_and_labels_2.csv\") # << Librosa\n",
        "# df = pd.read_csv(\"data/clip_features_and_labels_pyAudioAnalysis.csv\") # << pyAudioAnalysis\n",
        "\n",
        "df['label_sum'] = df[CLASSES].sum(axis=1)\n",
        "filtered_df = df[df['label_sum'] >= 1]\n",
        "filtered_df = filtered_df.drop(columns=['label_sum'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "a5T1PqpA7HZv"
      },
      "outputs": [],
      "source": [
        "### prep-split & predictions based on features\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# features and labels\n",
        "X = filtered_df[['chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7']]\n",
        "y = filtered_df[CLASSES]\n",
        "\n",
        "# splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# scaling for svm\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxlIqR-N7dpu",
        "outputId": "ac6caa89-2c51-4a1d-d278-44d7202561b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in labels (y_train):\n",
            " bouncy             0\n",
            "tekno              0\n",
            "warzone            0\n",
            "industrial         0\n",
            "non-techno-drop    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in features (X_train):\n",
            " chroma_stft    0\n",
            "rmse           0\n",
            "spec_cent      0\n",
            "spec_bw        0\n",
            "rolloff        0\n",
            "zcr            0\n",
            "mfcc1          0\n",
            "mfcc2          0\n",
            "mfcc3          0\n",
            "mfcc4          0\n",
            "mfcc5          0\n",
            "mfcc6          0\n",
            "mfcc7          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# purely for debugging during testing\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# checking for missing values\n",
        "missing_labels = y_train.isnull().sum()\n",
        "print(\"Missing values in labels (y_train):\\n\", missing_labels)\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train, columns=['chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'])\n",
        "missing_features = X_train_df.isnull().sum()\n",
        "print(\"\\nMissing values in features (X_train):\\n\", missing_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8tuJroeF0MS",
        "outputId": "996b750d-2159-44e0-a288-aa2c3d4f79c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics for bouncy:\n",
            "Accuracy:  0.8544061302681992\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.86      0.91       213\n",
            "           1       0.57      0.83      0.68        48\n",
            "\n",
            "    accuracy                           0.85       261\n",
            "   macro avg       0.76      0.85      0.79       261\n",
            "weighted avg       0.89      0.85      0.86       261\n",
            "\n",
            "\n",
            "Metrics for tekno:\n",
            "Accuracy:  0.8275862068965517\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.84      0.89       208\n",
            "           1       0.55      0.79      0.65        53\n",
            "\n",
            "    accuracy                           0.83       261\n",
            "   macro avg       0.75      0.81      0.77       261\n",
            "weighted avg       0.86      0.83      0.84       261\n",
            "\n",
            "\n",
            "Metrics for warzone:\n",
            "Accuracy:  0.8390804597701149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.84      0.89       209\n",
            "           1       0.56      0.85      0.68        52\n",
            "\n",
            "    accuracy                           0.84       261\n",
            "   macro avg       0.76      0.84      0.78       261\n",
            "weighted avg       0.88      0.84      0.85       261\n",
            "\n",
            "\n",
            "Metrics for industrial:\n",
            "Accuracy:  0.789272030651341\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.78      0.85       200\n",
            "           1       0.53      0.84      0.65        61\n",
            "\n",
            "    accuracy                           0.79       261\n",
            "   macro avg       0.74      0.81      0.75       261\n",
            "weighted avg       0.84      0.79      0.80       261\n",
            "\n",
            "\n",
            "Metrics for non-techno-drop:\n",
            "Accuracy:  0.9808429118773946\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       214\n",
            "           1       0.90      1.00      0.95        47\n",
            "\n",
            "    accuracy                           0.98       261\n",
            "   macro avg       0.95      0.99      0.97       261\n",
            "weighted avg       0.98      0.98      0.98       261\n",
            "\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.57      0.83      0.68        48\n",
            "          tekno       0.55      0.79      0.65        53\n",
            "        warzone       0.56      0.85      0.68        52\n",
            "     industrial       0.53      0.84      0.65        61\n",
            "non-techno-drop       0.90      1.00      0.95        47\n",
            "\n",
            "      micro avg       0.60      0.86      0.71       261\n",
            "      macro avg       0.62      0.86      0.72       261\n",
            "   weighted avg       0.62      0.86      0.71       261\n",
            "    samples avg       0.68      0.86      0.74       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new SVM for single-label\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "svm = OneVsRestClassifier(\n",
        "        SVC(kernel='rbf', probability=False, class_weight='balanced'))\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "for i, label in enumerate(y.columns):\n",
        "    print(f\"\\nMetrics for {label}:\")\n",
        "    print(\"Accuracy: \", accuracy_score(y_test.iloc[:, i], y_pred[:, i]))\n",
        "    print(classification_report(y_test.iloc[:, i], y_pred[:, i]))\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=CLASSES))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P6wICD7CEp3",
        "outputId": "4cfeaeee-829b-4ef1-e13d-0cb986c10a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.47081\tval-mlogloss:1.50340\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50]\ttrain-mlogloss:0.17770\tval-mlogloss:0.71909\n",
            "[99]\ttrain-mlogloss:0.06384\tval-mlogloss:0.71683\n",
            "\n",
            "=== Overall report on XGBoost validation set ===\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.63      0.69      0.66        48\n",
            "          tekno       0.68      0.68      0.68        53\n",
            "        warzone       0.73      0.79      0.76        52\n",
            "     industrial       0.76      0.64      0.70        61\n",
            "non-techno-drop       0.92      0.96      0.94        47\n",
            "\n",
            "       accuracy                           0.74       261\n",
            "      macro avg       0.75      0.75      0.75       261\n",
            "   weighted avg       0.74      0.74      0.74       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new XGBoost for single-label\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd, xgboost as xgb, numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "Dy_train = np.asarray(y_train)\n",
        "Dy_test  = np.asarray(y_test)\n",
        "\n",
        "Dy_train_idx = np.argmax(Dy_train, axis=1)\n",
        "Dy_test_idx  = np.argmax(Dy_test,  axis=1)\n",
        "\n",
        "num_class = Dy_train.shape[1] \n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=Dy_train_idx)\n",
        "dval   = xgb.DMatrix(X_test,  label=Dy_test_idx)\n",
        "\n",
        "params = dict(\n",
        "    objective     = 'multi:softprob',\n",
        "    num_class     = num_class,\n",
        "    eval_metric   = 'mlogloss',\n",
        "    max_depth     = 6,\n",
        "    eta           = 0.1,\n",
        "    subsample     = 0.9,\n",
        "    colsample_bytree = 0.9\n",
        ")\n",
        "\n",
        "booster = xgb.train(params, dtrain,\n",
        "                    num_boost_round=600,\n",
        "                    evals=[(dtrain,'train'), (dval,'val')],\n",
        "                    early_stopping_rounds=30,\n",
        "                    verbose_eval=50)\n",
        "\n",
        "\n",
        "y_pred = booster.predict(dval).argmax(axis=1)\n",
        "\n",
        "#  overall multi class report\n",
        "print(\"\\n=== Overall report on XGBoost validation set ===\")\n",
        "print(classification_report(Dy_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ZeRPbZVs-b",
        "outputId": "1ec51b47-302a-4255-b3f9-2e708952a172"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGB  macro-F1: 0.7675351811960717 \n",
            "Params: {'tree_method': 'hist', 'subsample': 0.85, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 8, 'lambda_l2': 1.0, 'lambda_l1': 1.0, 'gamma': 0.5, 'eta': 0.03, 'colsample_bytree': 0.6}\n"
          ]
        }
      ],
      "source": [
        "### hyper-tunning XGBoost\n",
        "\n",
        "\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'max_depth'        : [4, 6, 8, 10],\n",
        "    'eta'              : [0.30, 0.15, 0.07, 0.03],\n",
        "    'n_estimators'     : [200, 400, 800, 1200],\n",
        "    'subsample'        : [0.7, 0.85, 1.0],\n",
        "    'colsample_bytree' : [0.6, 0.8, 1.0],\n",
        "    'min_child_weight' : [1, 3, 5],\n",
        "    'gamma'            : [0, 0.5, 1],\n",
        "    'lambda_l1'        : [0,  1.0],\n",
        "    'lambda_l2'        : [0,  1.0],\n",
        "    'tree_method'      : ['hist'],\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.metrics import f1_score\n",
        "search = list(ParameterSampler(param_grid_xgb, n_iter=25, random_state=SEED))\n",
        "\n",
        "best_macro_f1 = 0\n",
        "best_params   = None\n",
        "\n",
        "for p in search:\n",
        "    model = xgb.XGBClassifier(objective='multi:softprob',\n",
        "                              num_class=len(CLASSES),\n",
        "                              eval_metric='mlogloss',\n",
        "                              early_stopping_rounds=50,\n",
        "                              **p)\n",
        "    model.fit(X_train, Dy_train_idx,\n",
        "              eval_set=[(X_test, Dy_test_idx)],\n",
        "              verbose=False)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(Dy_test_idx, y_pred, average='macro')\n",
        "    if f1 > best_macro_f1:\n",
        "        best_macro_f1, best_params = f1, p\n",
        "\n",
        "print(\"Best XGB  macro-F1:\", best_macro_f1, \"\\nParams:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YC_7yKXXL86",
        "outputId": "7f704bed-e04b-4bf5-9d08-af9d6147e3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.56760\tval-mlogloss:1.57687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [01:32:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50]\ttrain-mlogloss:0.61683\tval-mlogloss:0.94030\n",
            "[100]\ttrain-mlogloss:0.33241\tval-mlogloss:0.76317\n",
            "[150]\ttrain-mlogloss:0.22592\tval-mlogloss:0.71270\n",
            "[200]\ttrain-mlogloss:0.18648\tval-mlogloss:0.69622\n",
            "[250]\ttrain-mlogloss:0.17249\tval-mlogloss:0.69185\n",
            "[300]\ttrain-mlogloss:0.16464\tval-mlogloss:0.68935\n",
            "[350]\ttrain-mlogloss:0.16056\tval-mlogloss:0.68896\n",
            "[351]\ttrain-mlogloss:0.16037\tval-mlogloss:0.68916\n",
            "for *TUNED* XGBoost\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.66      0.65      0.65        48\n",
            "          tekno       0.67      0.72      0.69        53\n",
            "        warzone       0.75      0.83      0.79        52\n",
            "     industrial       0.82      0.67      0.74        61\n",
            "non-techno-drop       0.92      0.98      0.95        47\n",
            "\n",
            "       accuracy                           0.76       261\n",
            "      macro avg       0.76      0.77      0.76       261\n",
            "   weighted avg       0.76      0.76      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### *TUNED XGBoost for single-label\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd, xgboost as xgb, numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "Dy_train = np.asarray(y_train)\n",
        "Dy_test  = np.asarray(y_test)\n",
        "\n",
        "Dy_train_idx = np.argmax(Dy_train, axis=1)\n",
        "Dy_test_idx  = np.argmax(Dy_test,  axis=1)\n",
        "\n",
        "num_class = Dy_train.shape[1] \n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=Dy_train_idx)\n",
        "dval   = xgb.DMatrix(X_test,  label=Dy_test_idx)\n",
        "\n",
        "params = best_params.copy()\n",
        "params.pop(\"n_estimators\", None)\n",
        "params.update(dict(\n",
        "    objective   = 'multi:softprob',\n",
        "    num_class   = len(CLASSES),\n",
        "    eval_metric = 'mlogloss'\n",
        "))\n",
        "\n",
        "\n",
        "booster = xgb.train(params, dtrain,\n",
        "                    num_boost_round=600,\n",
        "                    evals=[(dtrain,'train'), (dval,'val')],\n",
        "                    early_stopping_rounds=30,\n",
        "                    verbose_eval=50)\n",
        "\n",
        "\n",
        "y_pred = booster.predict(dval).argmax(axis=1)\n",
        "\n",
        "#  overall multi class report\n",
        "print(\"for *TUNED* XGBoost\")\n",
        "print(classification_report(Dy_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "dk5WRZ6VN6xB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttrain's multi_logloss: 0.0463771\tval's multi_logloss: 0.728026\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttrain's multi_logloss: 0.125694\tval's multi_logloss: 0.710398\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.72      0.65      0.68        48\n",
            "          tekno       0.66      0.70      0.68        53\n",
            "        warzone       0.74      0.83      0.78        52\n",
            "     industrial       0.77      0.67      0.72        61\n",
            "non-techno-drop       0.90      0.98      0.94        47\n",
            "\n",
            "       accuracy                           0.76       261\n",
            "      macro avg       0.76      0.76      0.76       261\n",
            "   weighted avg       0.76      0.76      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new LightGBM for ML\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "y_train_idx = np.argmax(y_train, axis=1)\n",
        "y_test_idx  = np.argmax(y_test, axis=1)\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train_idx)\n",
        "val_set   = lgb.Dataset(X_test,   label=y_test_idx, reference=train_set)\n",
        "\n",
        "params_lgb = dict(\n",
        "    objective        = 'multiclass',\n",
        "    num_class        = len(CLASSES),\n",
        "    metric           = 'multi_logloss',\n",
        "    learning_rate    = 0.05,\n",
        "    num_leaves       = 31,\n",
        "    feature_fraction = 0.9,\n",
        "    bagging_fraction = 0.9,\n",
        "    bagging_freq     = 5,\n",
        "    seed             = SEED,\n",
        "    verbosity        = -1 \n",
        ")\n",
        "\n",
        "lgbm = lgb.train(\n",
        "        params_lgb,\n",
        "        train_set,\n",
        "        num_boost_round = 2000,\n",
        "        valid_sets      = [train_set, val_set],\n",
        "        valid_names     = ['train',  'val'],\n",
        "        callbacks       = [lgb.early_stopping(stopping_rounds=100,\n",
        "                                             verbose=True),\n",
        "                           lgb.log_evaluation(period=100)]\n",
        ")\n",
        "\n",
        "# prediction\n",
        "y_pred = np.argmax(lgbm.predict(X_test), axis=1)\n",
        "\n",
        "print(classification_report(y_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-78lHXdYSvF",
        "outputId": "906aaeb2-5df9-46ec-a348-8d725d3581fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best LGBM macro-F1: 0.7680522864037456\n",
            "Parameters: {'reg_lambda': 0, 'reg_alpha': 0, 'n_estimators': 400, 'min_split_gain': 0.2, 'min_child_samples': 10, 'max_depth': 8, 'learning_rate': 0.05, 'feature_fraction': 0.85, 'bagging_freq': 0, 'bagging_fraction': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "### hyper-tunning LightGBM\n",
        "\n",
        "\n",
        "\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth'        : [4, 6, 8, -1],\n",
        "    'learning_rate'    : [0.20, 0.10, 0.05],\n",
        "    'n_estimators'     : [400, 800, 1600],\n",
        "    'feature_fraction' : [0.7, 0.85, 1.0],\n",
        "    'bagging_fraction' : [0.7, 0.85, 1.0],\n",
        "    'bagging_freq'     : [0, 5],\n",
        "    'min_child_samples': [10, 25, 50],\n",
        "    'min_split_gain'   : [0, 0.2],\n",
        "    'reg_alpha'        : [0, 0.5],\n",
        "    'reg_lambda'       : [0, 1.0]\n",
        "}\n",
        "\n",
        "search = ParameterSampler(param_grid, n_iter=25, random_state=SEED)\n",
        "\n",
        "best_f1, best_params = 0, None\n",
        "for p in search:\n",
        "    clf = LGBMClassifier(objective='multiclass',\n",
        "                         num_class=len(CLASSES),\n",
        "                         **p)\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train_idx,\n",
        "        eval_set=[(X_test, y_test_idx)],\n",
        "        eval_metric='multi_logloss',\n",
        "        callbacks=[early_stopping(50, verbose=False),\n",
        "                   log_evaluation(period=0)]\n",
        "    )\n",
        "\n",
        "    f1 = f1_score(y_test_idx, clf.predict(X_test), average='macro')\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_params = f1, p\n",
        "\n",
        "print(\"Best LGBM macro-F1:\", best_f1)\n",
        "print(\"Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhpxaoQacq4g",
        "outputId": "fcd571f9-388c-4930-e301-45d68d805263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttrain's multi_logloss: 0.0704831\tval's multi_logloss: 0.714505\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttrain's multi_logloss: 0.116869\tval's multi_logloss: 0.706085\n",
            "for *TUNED* LightGBM\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.70      0.69      0.69        48\n",
            "          tekno       0.70      0.70      0.70        53\n",
            "        warzone       0.74      0.81      0.77        52\n",
            "     industrial       0.76      0.67      0.71        61\n",
            "non-techno-drop       0.94      1.00      0.97        47\n",
            "\n",
            "       accuracy                           0.77       261\n",
            "      macro avg       0.77      0.77      0.77       261\n",
            "   weighted avg       0.76      0.77      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### *TUNED LightGBM\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train_idx)\n",
        "val_set   = lgb.Dataset(X_test,   label=y_test_idx, reference=train_set)\n",
        "\n",
        "params_lgb = best_params.copy()\n",
        "params_lgb.update({\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(CLASSES),\n",
        "    'metric': 'multi_logloss',\n",
        "    'seed': SEED,\n",
        "    'verbosity': -1 \n",
        "})\n",
        "\n",
        "num_boost_round = params_lgb.pop('n_estimators')\n",
        "\n",
        "#  training with callbacks\n",
        "lgbm = lgb.train(\n",
        "    params_lgb,\n",
        "    train_set,\n",
        "    num_boost_round = 1600,\n",
        "    valid_sets      = [train_set, val_set],\n",
        "    valid_names     = ['train', 'val'],\n",
        "    callbacks       = [\n",
        "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
        "        lgb.log_evaluation(period=100)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# prediction\n",
        "y_pred = np.argmax(lgbm.predict(X_test), axis=1)\n",
        "\n",
        "print(\"for *TUNED* LightGBM\")\n",
        "print(classification_report(y_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zyIVAfEoeoWM",
        "outputId": "3c18bc08-add8-4723-bc4d-49328cf76a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:32:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.71      0.67      0.69        48\n",
            "          tekno       0.66      0.70      0.68        53\n",
            "        warzone       0.77      0.83      0.80        52\n",
            "     industrial       0.78      0.69      0.73        61\n",
            "non-techno-drop       0.92      0.98      0.95        47\n",
            "\n",
            "       accuracy                           0.77       261\n",
            "      macro avg       0.77      0.77      0.77       261\n",
            "   weighted avg       0.77      0.77      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### soft-vote stacking\n",
        "\n",
        "\n",
        "\n",
        "# xgboost\n",
        "params_xgb = params.copy()\n",
        "params_xgb.update({\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': len(CLASSES),\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'seed': SEED\n",
        "})\n",
        "\n",
        "dtrain_xgb = xgb.DMatrix(X_train, label=y_train_idx)\n",
        "dtest_xgb  = xgb.DMatrix(X_test)\n",
        "\n",
        "booster_xgb = xgb.train(params_xgb, dtrain_xgb, num_boost_round=200)\n",
        "\n",
        "# lgbm\n",
        "params_lgbm = params_lgb.copy()  \n",
        "params_lgbm.update({\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(CLASSES),\n",
        "    'metric': 'multi_logloss',\n",
        "    'seed': SEED,\n",
        "    'verbosity': -1  \n",
        "})\n",
        "\n",
        "train_set_lgb = lgb.Dataset(X_train, label=y_train_idx)\n",
        "booster_lgb = lgb.train(params_lgbm,\n",
        "                        train_set_lgb,\n",
        "                        num_boost_round=400)\n",
        "\n",
        "# pedict \n",
        "proba_xgb = booster_xgb.predict(dtest_xgb)\n",
        "proba_lgb = booster_lgb.predict(X_test)\n",
        "\n",
        "# soft-vote stacking\n",
        "alpha = 0.5 \n",
        "proba_stack = alpha * proba_xgb + (1 - alpha) * proba_lgb\n",
        "y_pred_stack = proba_stack.argmax(axis=1)\n",
        "\n",
        "print(classification_report(y_test_idx, y_pred_stack, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f1DxQfyE2Mwm",
        "outputId": "eacae3ac-47f3-493d-cb32-cbe6fcb4f4e8"
      },
      "outputs": [],
      "source": [
        "# ### creating normal melgrams\n",
        "\n",
        "\n",
        "\n",
        "# import librosa\n",
        "# import librosa.display\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# SR      = 22_050\n",
        "# N_MELS  = 128\n",
        "# F_CUT   = 8_000\n",
        "\n",
        "# def save_melgram(input_path, output_path):\n",
        "\n",
        "#     # loading the audio file\n",
        "#     y, sr = librosa.load(input_path, sr=22_050)\n",
        "\n",
        "#     # generating the mel spectograms\n",
        "#     S = librosa.feature.melspectrogram(y=y, sr=sr,\n",
        "#                                        n_fft=1024,\n",
        "#                                        hop_length=512,\n",
        "#                                        n_mels=128,\n",
        "#                                        fmin=20,\n",
        "#                                        fmax=11_000)\n",
        "#     S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "#     # plotting the spectograms\n",
        "#     plt.figure(figsize=(3, 3))\n",
        "#     librosa.display.specshow(S_db, sr=sr, hop_length=512,\n",
        "#                              x_axis='time', y_axis='mel',\n",
        "#                              fmax=11_000, cmap='gray_r')   # grayscale\n",
        "\n",
        "#     plt.axis('off')  # removing the axis\n",
        "#     plt.tight_layout(pad=0)\n",
        "\n",
        "#     # saving\n",
        "#     plt.savefig(output_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close()\n",
        "\n",
        "# # going through the clips, to make the spectograms\n",
        "# for filename in os.listdir(clips_dir):\n",
        "#     if filename.endswith(\".mp3\"):\n",
        "#         clip_path = os.path.join(clips_dir, filename)\n",
        "#         melgram_path = os.path.join(melgrams_dir, filename[:-4] + \".png\")\n",
        "#         save_melgram(clip_path, melgram_path)\n",
        "#         print(f\"Melgram saved for {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2BaQtieXq9HI",
        "outputId": "30a5d846-3c77-44b1-cf2f-7bf4c06f2c73"
      },
      "outputs": [],
      "source": [
        "# ### creating melgrams with low-pass filter\n",
        "\n",
        "\n",
        "\n",
        "# import librosa\n",
        "# import librosa.display\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# SR      = 22_050\n",
        "# N_MELS  = 128\n",
        "# F_CUT   = 2_000        # low-pass corner\n",
        "\n",
        "# def save_melgram(input_path, output_path):\n",
        "\n",
        "#     # loading the audio file\n",
        "#     y, _ = librosa.load(input_path, sr=SR)\n",
        "\n",
        "#     # generating the mel spectograms\n",
        "\n",
        "#     M = librosa.feature.melspectrogram(\n",
        "#             y=y, sr=SR,\n",
        "#             n_fft=1024,\n",
        "#             hop_length=512,\n",
        "#             n_mels=N_MELS,\n",
        "#             fmin=20,\n",
        "#             fmax=F_CUT)\n",
        "\n",
        "#     M_db = librosa.power_to_db(M, ref=np.max).astype(np.float32)\n",
        "\n",
        "#     # plot PNG > grayscale\n",
        "#     plt.figure(figsize=(3, 3))\n",
        "#     librosa.display.specshow(M_db, sr=SR,\n",
        "#                              hop_length=512,\n",
        "#                              x_axis='time', y_axis='mel',\n",
        "#                              fmax=F_CUT,\n",
        "#                              cmap='gray_r')\n",
        "#     plt.axis('off')\n",
        "#     plt.tight_layout(pad=0)\n",
        "#     plt.savefig(output_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close()\n",
        "\n",
        "# # going through the clips, to make the spectograms\n",
        "# for filename in os.listdir(clips_dir):\n",
        "#     if filename.endswith(\".mp3\"):\n",
        "#         clip_path = os.path.join(clips_dir, filename)\n",
        "#         low_melgram_path = os.path.join(low_melgrams_dir, filename[:-4] + \".png\")\n",
        "#         save_melgram(clip_path, low_melgram_path)\n",
        "#         print(f\"Melgram saved for {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzSw_DSU759L",
        "outputId": "762fb799-1702-42fd-acef-129013e6a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/low_melgrams\n",
            "1772\n"
          ]
        }
      ],
      "source": [
        "### see how many we melgrams we got after all\n",
        "#1772 ?\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "n_written = sum(1 for f in os.listdir(low_melgrams_dir))\n",
        "print(low_melgrams_dir)\n",
        "print(n_written)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATGOcyMLcV4e",
        "outputId": "61ef0aea-40b1-4c01-a041-54275d8a437c"
      },
      "outputs": [],
      "source": [
        "# ### connect to gdrive\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !ls \"/content/drive/MyDrive/MSc/deep_learning/apostolis/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "LVKqp3f41lmk"
      },
      "outputs": [],
      "source": [
        "### setting input files and folder-dirs\n",
        "\n",
        "\n",
        "\n",
        "CLASSES = ['bouncy','tekno','warzone','industrial','non-techno-drop']\n",
        "# REMOVED: 'psytech',\n",
        "\n",
        "# csv_file_path = \"/content/drive/MyDrive/MSc/deep_learning/labels_final.csv\"\n",
        "# tracks_dir = \"/content/drive/MyDrive/MSc/deep_learning/tracks_final\"\n",
        "# clips_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/clips\"\n",
        "\n",
        "# norm_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/norm_melgrams_dir\"\n",
        "# low_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/low_melgrams\"\n",
        "# melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "csv_file_path = \"data/labels_final.csv\"\n",
        "tracks_dir = \"data/tracks_final\"\n",
        "clips_dir = \"data/clips\"\n",
        "\n",
        "norm_melgrams_dir = \"data/melgrams\"\n",
        "low_melgrams_dir = \"data/low_melgrams\"\n",
        "melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "import os\n",
        "os.makedirs(clips_dir, exist_ok=True)\n",
        "os.makedirs(norm_melgrams_dir, exist_ok=True)\n",
        "os.makedirs(low_melgrams_dir, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# import random, numpy as np\n",
        "# from tensorflow.keras.utils import set_random_seed\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# tf.random.set_seed(SEED)\n",
        "# set_random_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "hyyDl2ApO-Ll"
      },
      "outputs": [],
      "source": [
        "### mapping labels to melgrams\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# loading the csv with the labels\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "# creating a dictionary to keep the mapping\n",
        "melgram_label_mapping = {}\n",
        "\n",
        "# going through the csv\n",
        "for index, row in df_csv.iterrows():\n",
        "    sample_id = str(row['sample_id'])\n",
        "    melgram_filename = sample_id + \".png\"\n",
        "\n",
        "    labels = row[CLASSES].values.tolist()\n",
        "\n",
        "    # check to ensure at least one label is 1\n",
        "    if any(label == 1 for label in labels):\n",
        "        melgram_label_mapping[melgram_filename] = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "NTaT2oGjxmyx",
        "outputId": "48ba80c0-dd9d-4205-ca10-7da64ddcb417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tensors: (1305, 128, 128, 1)\n",
            "\n",
            "Train clips : 872\n",
            "Valid clips : 207\n",
            "Test  clips : 226\n",
            "\n",
            "Train labels : 872\n",
            "Valid labels : 207\n",
            "Test  labels : 226\n"
          ]
        }
      ],
      "source": [
        "# prep-split for train/val/test\n",
        "\n",
        "\n",
        "\n",
        "# build X, y + a 'groups' list with song IDs, to ensure no same-song clips fall in different train/val/test sets !\n",
        "\n",
        "from pathlib import Path\n",
        "import cv2, numpy as np, pandas as pd, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "image_size = (128, 128)\n",
        "input_shape = (128, 128, 1)\n",
        "labels_list = CLASSES\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path).set_index('sample_id')\n",
        "\n",
        "def list_to_idx(vec):\n",
        "    idxs = [i for i, v in enumerate(vec) if v == 1]\n",
        "    return idxs[0] if len(idxs) == 1 else None\n",
        "\n",
        "X, y_int, groups = [], [], []\n",
        "\n",
        "for fname, vec in melgram_label_mapping.items():\n",
        "    class_idx = list_to_idx(vec)\n",
        "    if class_idx is None:\n",
        "        continue\n",
        "\n",
        "    png_path = os.path.join(melgrams_dir, fname)\n",
        "    if not os.path.exists(png_path):\n",
        "        continue\n",
        "\n",
        "    # load & preprocess to (128,128,1)\n",
        "    img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, image_size, interpolation=cv2.INTER_AREA)\n",
        "    img = np.expand_dims(img, -1).astype(np.float32) / 255.0\n",
        "\n",
        "    X.append(img)\n",
        "    y_int.append(class_idx)\n",
        "\n",
        "    sample_id = Path(fname).stem\n",
        "    groups.append(df_csv.loc[sample_id, 'spot_id'])\n",
        "\n",
        "X       = np.array(X,       dtype=np.float32)\n",
        "y_int   = np.array(y_int,  dtype=np.int32)\n",
        "groups  = np.array(groups)\n",
        "\n",
        "print(\"Loaded tensors:\", X.shape)\n",
        "\n",
        "# first split, 70% train, 30% temp (val+test)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "train_idx, temp_idx = next(sgkf.split(X, y_int, groups=groups))\n",
        "\n",
        "# 2nd split, 50/50 on temp â†’ 15% val, 15% test\n",
        "\n",
        "sgkf_temp = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
        "val_idx, test_idx = next(sgkf_temp.split(X[temp_idx], y_int[temp_idx], groups=groups[temp_idx]))\n",
        "\n",
        "X_train, y_train = X[train_idx], y_int[train_idx]\n",
        "X_val, y_val = X[temp_idx][val_idx], y_int[temp_idx][val_idx]\n",
        "X_test, y_test = X[temp_idx][test_idx], y_int[temp_idx][test_idx]\n",
        "\n",
        "print(f\"\\nTrain clips : {len(X_train)}\")\n",
        "print(f\"Valid clips : {len(X_val)}\")\n",
        "print(f\"Test  clips : {len(X_test)}\")\n",
        "\n",
        "print(f\"\\nTrain labels : {len(y_train)}\")\n",
        "print(f\"Valid labels : {len(y_val)}\")\n",
        "print(f\"Test  labels : {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "collapsed": true,
        "id": "JQ9R7FbwHutH"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9mVIOskGu3e",
        "outputId": "7ee97b8c-3538-4223-bb13-0b16690173cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in y_train: [0 1 2 3 4]\n",
            "X_train shape: (872, 128, 128, 1)\n",
            "y_train shape: (872,)\n",
            "Sample y_train: 3\n",
            "y_train dtype: int32\n"
          ]
        }
      ],
      "source": [
        "### train-set check + build metric-printers\n",
        "\n",
        "\n",
        "\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"Sample y_train:\", y_train[0])\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# evaluating the model on the validation & the test set\n",
        "\n",
        "def show_val_results(history, X_val, y_val, class_names):\n",
        "\n",
        "    # epoch with the lowest val-loss\n",
        "\n",
        "    best = np.argmin(history.history['val_loss'])\n",
        "    print(f\"\\nBest epoch = {best+1}\")\n",
        "    print(f\"  train_acc {history.history['accuracy'][best]:.4f}\"\n",
        "          f\" | train_loss {history.history['loss'][best]:.4f}\")\n",
        "    print(f\"  val_acc   {history.history['val_accuracy'][best]:.4f}\"\n",
        "          f\" | val_loss  {history.history['val_loss'][best]:.4f}\")\n",
        "\n",
        "    y_pred = history.model.predict(X_val, verbose=0).argmax(axis=1)\n",
        "    print(f\"\\nWeighted F1 (val) : {f1_score(y_val, y_pred, average='weighted'):.4f}\")\n",
        "    print(classification_report(y_val, y_pred, target_names=class_names))\n",
        "    print(\"Confusion matrix (val):\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "\n",
        "def show_test_results(modelo, X_test, y_test, class_names):\n",
        "\n",
        "    y_pred = modelo.predict(X_test, verbose=0).argmax(axis=1)\n",
        "\n",
        "    print(\"\\n=== TEST-SET RESULTS ===\")\n",
        "    print(f\"Weighted F1 (test): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "    print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-63NhDCOvUtI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train class counts: {'bouncy': np.int64(184), 'tekno': np.int64(183), 'warzone': np.int64(177), 'industrial': np.int64(170), 'non-techno-drop': np.int64(158)}\n",
            "Val class counts: {'bouncy': np.int64(35), 'tekno': np.int64(30), 'warzone': np.int64(45), 'industrial': np.int64(47), 'non-techno-drop': np.int64(50)}\n",
            "Test class counts: {'bouncy': np.int64(42), 'tekno': np.int64(46), 'warzone': np.int64(44), 'industrial': np.int64(42), 'non-techno-drop': np.int64(52)}\n"
          ]
        }
      ],
      "source": [
        "### check & verify class distributions\n",
        "\n",
        "\n",
        "def print_class_dist(y, name):\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    print(f\"{name} class counts:\", dict(zip(labels_list, counts)))\n",
        "\n",
        "print_class_dist(y_train, \"Train\")\n",
        "print_class_dist(y_val, \"Val\")\n",
        "print_class_dist(y_test, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "j2amEnqMvUtI"
      },
      "outputs": [],
      "source": [
        "### class weight to handle class imbalance, especially in the val & test sets\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b-3W_5yqHnGM",
        "outputId": "c9b20aec-6076-403c-eea7-3443c582b599"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\me\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "14/14 - 8s - 606ms/step - accuracy: 0.2271 - loss: 1.6603 - val_accuracy: 0.1691 - val_loss: 1.6154\n",
            "Epoch 2/50\n",
            "14/14 - 7s - 487ms/step - accuracy: 0.3142 - loss: 1.5432 - val_accuracy: 0.4058 - val_loss: 1.4294\n",
            "Epoch 3/50\n",
            "14/14 - 7s - 495ms/step - accuracy: 0.4243 - loss: 1.3503 - val_accuracy: 0.5942 - val_loss: 1.1910\n",
            "Epoch 4/50\n",
            "14/14 - 8s - 545ms/step - accuracy: 0.5057 - loss: 1.2455 - val_accuracy: 0.5845 - val_loss: 1.1482\n",
            "Epoch 5/50\n",
            "14/14 - 8s - 567ms/step - accuracy: 0.5906 - loss: 1.1012 - val_accuracy: 0.6377 - val_loss: 1.0342\n",
            "Epoch 6/50\n",
            "14/14 - 7s - 489ms/step - accuracy: 0.6433 - loss: 0.9930 - val_accuracy: 0.6473 - val_loss: 0.9685\n",
            "Epoch 7/50\n",
            "14/14 - 7s - 494ms/step - accuracy: 0.6525 - loss: 0.8847 - val_accuracy: 0.6860 - val_loss: 0.9092\n",
            "Epoch 8/50\n",
            "14/14 - 7s - 476ms/step - accuracy: 0.7064 - loss: 0.8247 - val_accuracy: 0.7198 - val_loss: 0.8456\n",
            "Epoch 9/50\n",
            "14/14 - 7s - 484ms/step - accuracy: 0.7603 - loss: 0.7005 - val_accuracy: 0.7391 - val_loss: 0.7190\n",
            "Epoch 10/50\n",
            "14/14 - 7s - 485ms/step - accuracy: 0.7592 - loss: 0.6855 - val_accuracy: 0.7101 - val_loss: 0.8022\n",
            "Epoch 11/50\n",
            "14/14 - 7s - 487ms/step - accuracy: 0.7959 - loss: 0.5697 - val_accuracy: 0.7246 - val_loss: 0.7449\n",
            "Epoch 12/50\n",
            "14/14 - 7s - 466ms/step - accuracy: 0.8257 - loss: 0.5299 - val_accuracy: 0.7150 - val_loss: 0.7834\n",
            "Epoch 13/50\n",
            "14/14 - 7s - 478ms/step - accuracy: 0.8154 - loss: 0.5221 - val_accuracy: 0.7391 - val_loss: 0.7600\n",
            "Epoch 14/50\n",
            "14/14 - 7s - 469ms/step - accuracy: 0.8440 - loss: 0.4831 - val_accuracy: 0.7826 - val_loss: 0.6865\n",
            "Epoch 15/50\n",
            "14/14 - 7s - 472ms/step - accuracy: 0.8670 - loss: 0.3637 - val_accuracy: 0.8116 - val_loss: 0.5904\n",
            "Epoch 16/50\n",
            "14/14 - 7s - 497ms/step - accuracy: 0.8807 - loss: 0.3445 - val_accuracy: 0.8213 - val_loss: 0.5972\n",
            "Epoch 17/50\n",
            "14/14 - 7s - 509ms/step - accuracy: 0.8853 - loss: 0.3349 - val_accuracy: 0.7681 - val_loss: 0.6521\n",
            "Epoch 18/50\n",
            "14/14 - 7s - 489ms/step - accuracy: 0.8716 - loss: 0.3534 - val_accuracy: 0.8116 - val_loss: 0.5972\n",
            "Epoch 19/50\n",
            "14/14 - 7s - 489ms/step - accuracy: 0.8899 - loss: 0.3282 - val_accuracy: 0.7778 - val_loss: 0.6191\n",
            "Epoch 20/50\n",
            "14/14 - 7s - 473ms/step - accuracy: 0.9106 - loss: 0.2682 - val_accuracy: 0.7826 - val_loss: 0.6072\n",
            "Epoch 21/50\n",
            "14/14 - 7s - 475ms/step - accuracy: 0.9186 - loss: 0.2422 - val_accuracy: 0.7681 - val_loss: 0.6353\n",
            "Epoch 22/50\n",
            "14/14 - 7s - 480ms/step - accuracy: 0.9381 - loss: 0.1957 - val_accuracy: 0.7440 - val_loss: 0.7731\n",
            "Epoch 23/50\n",
            "14/14 - 7s - 483ms/step - accuracy: 0.9335 - loss: 0.2132 - val_accuracy: 0.8213 - val_loss: 0.6098\n",
            "Epoch 24/50\n",
            "14/14 - 7s - 521ms/step - accuracy: 0.9381 - loss: 0.1756 - val_accuracy: 0.7971 - val_loss: 0.6174\n",
            "Epoch 25/50\n",
            "14/14 - 7s - 489ms/step - accuracy: 0.9622 - loss: 0.1284 - val_accuracy: 0.7488 - val_loss: 0.6879\n",
            "\n",
            "Best epoch = 15\n",
            "  train_acc 0.8670 | train_loss 0.3637\n",
            "  val_acc   0.8116 | val_loss  0.5904\n",
            "\n",
            "Weighted F1 (val) : 0.8085\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.86      0.69      0.76        35\n",
            "          tekno       0.71      0.67      0.69        30\n",
            "        warzone       0.75      0.93      0.83        45\n",
            "     industrial       0.79      0.72      0.76        47\n",
            "non-techno-drop       0.92      0.96      0.94        50\n",
            "\n",
            "       accuracy                           0.81       207\n",
            "      macro avg       0.81      0.79      0.80       207\n",
            "   weighted avg       0.81      0.81      0.81       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[24  7  0  4  0]\n",
            " [ 3 20  3  3  1]\n",
            " [ 0  0 42  1  2]\n",
            " [ 1  0 11 34  1]\n",
            " [ 0  1  0  1 48]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.7478\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.94      0.71      0.81        42\n",
            "          tekno       0.62      0.70      0.65        46\n",
            "        warzone       0.59      0.75      0.66        44\n",
            "     industrial       0.69      0.60      0.64        42\n",
            "non-techno-drop       0.96      0.92      0.94        52\n",
            "\n",
            "       accuracy                           0.74       226\n",
            "      macro avg       0.76      0.74      0.74       226\n",
            "   weighted avg       0.76      0.74      0.75       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[30  1  2  8  1]\n",
            " [ 0 32 12  2  0]\n",
            " [ 0  9 33  1  1]\n",
            " [ 0 10  7 25  0]\n",
            " [ 2  0  2  0 48]]\n"
          ]
        }
      ],
      "source": [
        "### main CNN construction\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "    MaxPooling2D(2,2), # MAYBE (2,1) OR (1,2) ?\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels_list), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history1 = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "show_val_results(history1, X_val, y_val, CLASSES)\n",
        "show_test_results(model,   X_test, y_test, CLASSES)\n",
        "\n",
        "\n",
        "\n",
        "#################\n",
        "\n",
        "\n",
        "\n",
        "# results with main-CNN for normal melgrams after 5 runs, WITHOUT fixed tf.seed\n",
        "\n",
        "# run 1\n",
        "# Best epoch = 14\n",
        "#   train_acc 0.8647 | train_loss 0.3721\n",
        "#   val_acc   0.8068 | val_loss  0.6143\n",
        "# Weighted F1 (val) : 0.8050\n",
        "# Weighted F1 (test): 0.7621\n",
        "\n",
        "# run 2\n",
        "# Best epoch = 22\n",
        "#   train_acc 0.9472 | train_loss 0.1642\n",
        "#   val_acc   0.8454 | val_loss  0.5026\n",
        "# Weighted F1 (val) : 0.8448\n",
        "# Weighted F1 (test): 0.8101\n",
        "\n",
        "# run 3\n",
        "# Best epoch = 14\n",
        "#   train_acc 0.8784 | train_loss 0.3704\n",
        "#   val_acc   0.8502 | val_loss  0.5496\n",
        "# Weighted F1 (val) : 0.8482\n",
        "# Weighted F1 (test): 0.7641\n",
        "\n",
        "# run 4\n",
        "# Best epoch = 23\n",
        "#   train_acc 0.9025 | train_loss 0.2604\n",
        "#   val_acc   0.8213 | val_loss  0.5751\n",
        "# Weighted F1 (val) : 0.8134\n",
        "# Weighted F1 (test): 0.7938\n",
        "\n",
        "# run 5\n",
        "# Best epoch = 16\n",
        "#   train_acc 0.9071 | train_loss 0.2971\n",
        "#   val_acc   0.8696 | val_loss  0.5438\n",
        "# Weighted F1 (val) : 0.8668\n",
        "# Weighted F1 (test): 0.7904\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEj-RvBe4KJl",
        "outputId": "78bc5e35-bcf8-4803-83a5-78918304583f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\me\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 - 9s - 638ms/step - accuracy: 0.2156 - loss: 1.6491 - val_accuracy: 0.1691 - val_loss: 1.6048\n",
            "Epoch 2/50\n",
            "14/14 - 7s - 492ms/step - accuracy: 0.2557 - loss: 1.5814 - val_accuracy: 0.4106 - val_loss: 1.5598\n",
            "Epoch 3/50\n",
            "14/14 - 7s - 466ms/step - accuracy: 0.3647 - loss: 1.4739 - val_accuracy: 0.4300 - val_loss: 1.3819\n",
            "Epoch 4/50\n",
            "14/14 - 6s - 450ms/step - accuracy: 0.4025 - loss: 1.4000 - val_accuracy: 0.4589 - val_loss: 1.3706\n",
            "Epoch 5/50\n",
            "14/14 - 6s - 448ms/step - accuracy: 0.5344 - loss: 1.2225 - val_accuracy: 0.5072 - val_loss: 1.3086\n",
            "Epoch 6/50\n",
            "14/14 - 6s - 456ms/step - accuracy: 0.5241 - loss: 1.1788 - val_accuracy: 0.4976 - val_loss: 1.2101\n",
            "Epoch 7/50\n",
            "14/14 - 6s - 460ms/step - accuracy: 0.6330 - loss: 1.0466 - val_accuracy: 0.6570 - val_loss: 1.0564\n",
            "Epoch 8/50\n",
            "14/14 - 6s - 455ms/step - accuracy: 0.6571 - loss: 0.9567 - val_accuracy: 0.6184 - val_loss: 1.0543\n",
            "Epoch 9/50\n",
            "14/14 - 7s - 478ms/step - accuracy: 0.6892 - loss: 0.9157 - val_accuracy: 0.7101 - val_loss: 0.9384\n",
            "Epoch 10/50\n",
            "14/14 - 6s - 414ms/step - accuracy: 0.6961 - loss: 0.8401 - val_accuracy: 0.6087 - val_loss: 1.0029\n",
            "Epoch 11/50\n",
            "14/14 - 6s - 428ms/step - accuracy: 0.6823 - loss: 0.8578 - val_accuracy: 0.6232 - val_loss: 0.9439\n",
            "Epoch 12/50\n",
            "14/14 - 6s - 433ms/step - accuracy: 0.7351 - loss: 0.7251 - val_accuracy: 0.7053 - val_loss: 0.8735\n",
            "Epoch 13/50\n",
            "14/14 - 6s - 449ms/step - accuracy: 0.7557 - loss: 0.7009 - val_accuracy: 0.7053 - val_loss: 0.8485\n",
            "Epoch 14/50\n",
            "14/14 - 6s - 431ms/step - accuracy: 0.7833 - loss: 0.6482 - val_accuracy: 0.6473 - val_loss: 0.9450\n",
            "Epoch 15/50\n",
            "14/14 - 6s - 441ms/step - accuracy: 0.8085 - loss: 0.5576 - val_accuracy: 0.7440 - val_loss: 0.7525\n",
            "Epoch 16/50\n",
            "14/14 - 6s - 428ms/step - accuracy: 0.8222 - loss: 0.4809 - val_accuracy: 0.7246 - val_loss: 0.7406\n",
            "Epoch 17/50\n",
            "14/14 - 7s - 495ms/step - accuracy: 0.8440 - loss: 0.4628 - val_accuracy: 0.7198 - val_loss: 0.8235\n",
            "Epoch 18/50\n",
            "14/14 - 7s - 468ms/step - accuracy: 0.8349 - loss: 0.4621 - val_accuracy: 0.6232 - val_loss: 0.9519\n",
            "Epoch 19/50\n",
            "14/14 - 6s - 461ms/step - accuracy: 0.8073 - loss: 0.5185 - val_accuracy: 0.7536 - val_loss: 0.7718\n",
            "Epoch 20/50\n",
            "14/14 - 7s - 467ms/step - accuracy: 0.8624 - loss: 0.3848 - val_accuracy: 0.7150 - val_loss: 0.7440\n",
            "Epoch 21/50\n",
            "14/14 - 7s - 533ms/step - accuracy: 0.8876 - loss: 0.3422 - val_accuracy: 0.7633 - val_loss: 0.7227\n",
            "Epoch 22/50\n",
            "14/14 - 8s - 540ms/step - accuracy: 0.8716 - loss: 0.3529 - val_accuracy: 0.6715 - val_loss: 0.8275\n",
            "Epoch 23/50\n",
            "14/14 - 6s - 448ms/step - accuracy: 0.8337 - loss: 0.4094 - val_accuracy: 0.7391 - val_loss: 0.7211\n",
            "Epoch 24/50\n",
            "14/14 - 7s - 465ms/step - accuracy: 0.8773 - loss: 0.3565 - val_accuracy: 0.7488 - val_loss: 0.7164\n",
            "Epoch 25/50\n",
            "14/14 - 7s - 477ms/step - accuracy: 0.8979 - loss: 0.2825 - val_accuracy: 0.7633 - val_loss: 0.6967\n",
            "Epoch 26/50\n",
            "14/14 - 6s - 453ms/step - accuracy: 0.9128 - loss: 0.2543 - val_accuracy: 0.7391 - val_loss: 0.7010\n",
            "Epoch 27/50\n",
            "14/14 - 7s - 482ms/step - accuracy: 0.9117 - loss: 0.2487 - val_accuracy: 0.7440 - val_loss: 0.7276\n",
            "Epoch 28/50\n",
            "14/14 - 7s - 509ms/step - accuracy: 0.9381 - loss: 0.2079 - val_accuracy: 0.7488 - val_loss: 0.7940\n",
            "Epoch 29/50\n",
            "14/14 - 7s - 469ms/step - accuracy: 0.9243 - loss: 0.2104 - val_accuracy: 0.7150 - val_loss: 0.7919\n",
            "Epoch 30/50\n",
            "14/14 - 7s - 493ms/step - accuracy: 0.9404 - loss: 0.1894 - val_accuracy: 0.7633 - val_loss: 0.7361\n",
            "Epoch 31/50\n",
            "14/14 - 7s - 531ms/step - accuracy: 0.9622 - loss: 0.1576 - val_accuracy: 0.7536 - val_loss: 0.7232\n",
            "Epoch 32/50\n",
            "14/14 - 6s - 423ms/step - accuracy: 0.9450 - loss: 0.1627 - val_accuracy: 0.7923 - val_loss: 0.7028\n",
            "Epoch 33/50\n",
            "14/14 - 6s - 428ms/step - accuracy: 0.9530 - loss: 0.1444 - val_accuracy: 0.7440 - val_loss: 0.8506\n",
            "Epoch 34/50\n",
            "14/14 - 6s - 415ms/step - accuracy: 0.9415 - loss: 0.1690 - val_accuracy: 0.7488 - val_loss: 0.7815\n",
            "Epoch 35/50\n",
            "14/14 - 6s - 417ms/step - accuracy: 0.9518 - loss: 0.1405 - val_accuracy: 0.7681 - val_loss: 0.7949\n",
            "\n",
            "Best epoch = 25\n",
            "  train_acc 0.8979 | train_loss 0.2825\n",
            "  val_acc   0.7633 | val_loss  0.6967\n",
            "\n",
            "Weighted F1 (val) : 0.7649\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.82      0.66      0.73        35\n",
            "          tekno       0.58      0.60      0.59        30\n",
            "        warzone       0.83      0.87      0.85        45\n",
            "     industrial       0.66      0.79      0.72        47\n",
            "non-techno-drop       0.91      0.82      0.86        50\n",
            "\n",
            "       accuracy                           0.76       207\n",
            "      macro avg       0.76      0.75      0.75       207\n",
            "   weighted avg       0.77      0.76      0.76       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[23  5  0  7  0]\n",
            " [ 5 18  3  3  1]\n",
            " [ 0  0 39  4  2]\n",
            " [ 0  5  4 37  1]\n",
            " [ 0  3  1  5 41]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.7740\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.85      0.67      0.75        42\n",
            "          tekno       0.58      0.76      0.66        46\n",
            "        warzone       0.86      0.95      0.90        44\n",
            "     industrial       0.68      0.76      0.72        42\n",
            "non-techno-drop       1.00      0.71      0.83        52\n",
            "\n",
            "       accuracy                           0.77       226\n",
            "      macro avg       0.79      0.77      0.77       226\n",
            "   weighted avg       0.80      0.77      0.77       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[28  9  0  5  0]\n",
            " [ 0 35  5  6  0]\n",
            " [ 0  1 42  1  0]\n",
            " [ 0 10  0 32  0]\n",
            " [ 5  5  2  3 37]]\n"
          ]
        }
      ],
      "source": [
        "### new CNN Dropout>Flatten (2,2) ###\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# import tensorflow as tf\n",
        "# tf.random.set_seed(SEED)\n",
        "# tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(len(CLASSES), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "historyD22 = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "show_val_results(historyD22, X_val, y_val, CLASSES)\n",
        "show_test_results(model,   X_test, y_test, CLASSES)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################\n",
        "\n",
        "# results with new-CNN for normal melgrams after 5 runs, WITHOUT fixed tf.seed\n",
        "\n",
        "# run 1\n",
        "# Best epoch = 19\n",
        "#   train_acc 0.8784 | train_loss 0.3446\n",
        "#   val_acc   0.7729 | val_loss  0.6463\n",
        "# Weighted F1 (val) : 0.7778\n",
        "# Weighted F1 (test): 0.7538\n",
        "\n",
        "# run 2\n",
        "# Best epoch = 21\n",
        "#   train_acc 0.8911 | train_loss 0.3020\n",
        "#   val_acc   0.8019 | val_loss  0.6091\n",
        "# Weighted F1 (val) : 0.8000\n",
        "# Weighted F1 (test): 0.7799\n",
        "\n",
        "# run 3\n",
        "# Best epoch = 31\n",
        "#   train_acc 0.9323 | train_loss 0.1961\n",
        "#   val_acc   0.7971 | val_loss  0.6636\n",
        "# Weighted F1 (val) : 0.7954\n",
        "# Weighted F1 (test): 0.7888\n",
        "\n",
        "# run 4\n",
        "# Best epoch = 34\n",
        "#   train_acc 0.9656 | train_loss 0.1135\n",
        "#   val_acc   0.7874 | val_loss  0.6383\n",
        "# Weighted F1 (val) : 0.7837\n",
        "# Weighted F1 (test): 0.7956\n",
        "\n",
        "# run 5\n",
        "# Best epoch = 17\n",
        "#   train_acc 0.8532 | train_loss 0.4371\n",
        "#   val_acc   0.7440 | val_loss  0.7363\n",
        "# Weighted F1 (val) : 0.7460\n",
        "# Weighted F1 (test): 0.7390\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "LQnhb99i73wz",
        "outputId": "fa7965b1-ffd8-4809-e3d1-1f83339a829c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\me\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 - 8s - 537ms/step - accuracy: 0.1858 - loss: 1.7506 - val_accuracy: 0.1691 - val_loss: 1.6090\n",
            "Epoch 2/50\n",
            "14/14 - 6s - 425ms/step - accuracy: 0.2133 - loss: 1.6075 - val_accuracy: 0.1691 - val_loss: 1.6200\n",
            "Epoch 3/50\n",
            "14/14 - 6s - 420ms/step - accuracy: 0.2546 - loss: 1.5889 - val_accuracy: 0.1691 - val_loss: 1.6028\n",
            "Epoch 4/50\n",
            "14/14 - 8s - 594ms/step - accuracy: 0.3005 - loss: 1.5749 - val_accuracy: 0.2560 - val_loss: 1.5498\n",
            "Epoch 5/50\n",
            "14/14 - 7s - 521ms/step - accuracy: 0.2810 - loss: 1.5360 - val_accuracy: 0.2995 - val_loss: 1.5128\n",
            "Epoch 6/50\n",
            "14/14 - 7s - 504ms/step - accuracy: 0.4128 - loss: 1.4120 - val_accuracy: 0.4831 - val_loss: 1.4279\n",
            "Epoch 7/50\n",
            "14/14 - 6s - 458ms/step - accuracy: 0.5011 - loss: 1.2749 - val_accuracy: 0.5217 - val_loss: 1.2525\n",
            "Epoch 8/50\n",
            "14/14 - 6s - 440ms/step - accuracy: 0.5872 - loss: 1.1024 - val_accuracy: 0.4251 - val_loss: 1.2568\n",
            "Epoch 9/50\n",
            "14/14 - 6s - 423ms/step - accuracy: 0.5940 - loss: 1.0540 - val_accuracy: 0.5266 - val_loss: 1.1664\n",
            "Epoch 10/50\n",
            "14/14 - 6s - 419ms/step - accuracy: 0.6433 - loss: 0.9557 - val_accuracy: 0.5556 - val_loss: 1.1805\n",
            "Epoch 11/50\n",
            "14/14 - 6s - 434ms/step - accuracy: 0.6755 - loss: 0.8833 - val_accuracy: 0.5990 - val_loss: 0.9878\n",
            "Epoch 12/50\n",
            "14/14 - 6s - 432ms/step - accuracy: 0.6915 - loss: 0.8286 - val_accuracy: 0.7053 - val_loss: 0.9608\n",
            "Epoch 13/50\n",
            "14/14 - 7s - 464ms/step - accuracy: 0.7179 - loss: 0.7625 - val_accuracy: 0.6522 - val_loss: 0.9504\n",
            "Epoch 14/50\n",
            "14/14 - 6s - 445ms/step - accuracy: 0.7328 - loss: 0.7509 - val_accuracy: 0.5797 - val_loss: 1.0787\n",
            "Epoch 15/50\n",
            "14/14 - 6s - 421ms/step - accuracy: 0.7385 - loss: 0.7368 - val_accuracy: 0.5942 - val_loss: 1.0573\n",
            "Epoch 16/50\n",
            "14/14 - 6s - 448ms/step - accuracy: 0.7672 - loss: 0.6493 - val_accuracy: 0.6135 - val_loss: 1.0935\n",
            "Epoch 17/50\n",
            "14/14 - 6s - 434ms/step - accuracy: 0.7856 - loss: 0.5886 - val_accuracy: 0.6570 - val_loss: 0.9424\n",
            "Epoch 18/50\n",
            "14/14 - 6s - 418ms/step - accuracy: 0.7913 - loss: 0.5662 - val_accuracy: 0.6232 - val_loss: 1.0292\n",
            "Epoch 19/50\n",
            "14/14 - 6s - 418ms/step - accuracy: 0.8211 - loss: 0.5017 - val_accuracy: 0.6522 - val_loss: 0.9512\n",
            "Epoch 20/50\n",
            "14/14 - 6s - 418ms/step - accuracy: 0.8314 - loss: 0.4853 - val_accuracy: 0.6377 - val_loss: 0.9329\n",
            "Epoch 21/50\n",
            "14/14 - 6s - 416ms/step - accuracy: 0.8291 - loss: 0.4910 - val_accuracy: 0.6473 - val_loss: 0.9441\n",
            "Epoch 22/50\n",
            "14/14 - 6s - 419ms/step - accuracy: 0.8245 - loss: 0.4690 - val_accuracy: 0.6957 - val_loss: 0.8703\n",
            "Epoch 23/50\n",
            "14/14 - 6s - 428ms/step - accuracy: 0.8486 - loss: 0.4262 - val_accuracy: 0.6618 - val_loss: 0.8985\n",
            "Epoch 24/50\n",
            "14/14 - 6s - 424ms/step - accuracy: 0.8739 - loss: 0.3527 - val_accuracy: 0.7343 - val_loss: 0.8810\n",
            "Epoch 25/50\n",
            "14/14 - 6s - 453ms/step - accuracy: 0.8945 - loss: 0.3152 - val_accuracy: 0.6860 - val_loss: 0.9587\n",
            "Epoch 26/50\n",
            "14/14 - 6s - 432ms/step - accuracy: 0.9037 - loss: 0.2899 - val_accuracy: 0.7053 - val_loss: 0.8845\n",
            "Epoch 27/50\n",
            "14/14 - 6s - 429ms/step - accuracy: 0.9151 - loss: 0.2689 - val_accuracy: 0.7198 - val_loss: 0.8968\n",
            "Epoch 28/50\n",
            "14/14 - 6s - 426ms/step - accuracy: 0.9163 - loss: 0.2504 - val_accuracy: 0.7198 - val_loss: 0.8663\n",
            "Epoch 29/50\n",
            "14/14 - 6s - 432ms/step - accuracy: 0.9312 - loss: 0.2109 - val_accuracy: 0.7488 - val_loss: 0.8407\n",
            "Epoch 30/50\n",
            "14/14 - 6s - 428ms/step - accuracy: 0.9186 - loss: 0.2247 - val_accuracy: 0.7150 - val_loss: 0.9486\n",
            "Epoch 31/50\n",
            "14/14 - 6s - 424ms/step - accuracy: 0.9358 - loss: 0.1971 - val_accuracy: 0.7005 - val_loss: 0.9287\n",
            "Epoch 32/50\n",
            "14/14 - 6s - 430ms/step - accuracy: 0.9587 - loss: 0.1568 - val_accuracy: 0.7343 - val_loss: 0.9042\n",
            "Epoch 33/50\n",
            "14/14 - 6s - 420ms/step - accuracy: 0.9484 - loss: 0.1426 - val_accuracy: 0.7391 - val_loss: 0.8619\n",
            "Epoch 34/50\n",
            "14/14 - 6s - 431ms/step - accuracy: 0.9576 - loss: 0.1327 - val_accuracy: 0.7101 - val_loss: 1.0355\n",
            "Epoch 35/50\n",
            "14/14 - 6s - 436ms/step - accuracy: 0.9644 - loss: 0.1222 - val_accuracy: 0.7005 - val_loss: 1.0477\n",
            "Epoch 36/50\n",
            "14/14 - 7s - 515ms/step - accuracy: 0.9713 - loss: 0.1109 - val_accuracy: 0.7005 - val_loss: 1.0871\n",
            "Epoch 37/50\n",
            "14/14 - 6s - 423ms/step - accuracy: 0.9679 - loss: 0.1027 - val_accuracy: 0.7536 - val_loss: 0.9931\n",
            "Epoch 38/50\n",
            "14/14 - 6s - 431ms/step - accuracy: 0.9817 - loss: 0.0787 - val_accuracy: 0.6957 - val_loss: 1.1611\n",
            "Epoch 39/50\n",
            "14/14 - 6s - 431ms/step - accuracy: 0.9656 - loss: 0.1007 - val_accuracy: 0.7585 - val_loss: 1.0299\n",
            "\n",
            "Best epoch = 29\n",
            "  train_acc 0.9312 | train_loss 0.2109\n",
            "  val_acc   0.7488 | val_loss  0.8407\n",
            "\n",
            "Weighted F1 (val) : 0.7499\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.69      0.83      0.75        35\n",
            "          tekno       0.56      0.60      0.58        30\n",
            "        warzone       0.87      0.60      0.71        45\n",
            "     industrial       0.68      0.81      0.74        47\n",
            "non-techno-drop       0.93      0.86      0.90        50\n",
            "\n",
            "       accuracy                           0.75       207\n",
            "      macro avg       0.75      0.74      0.74       207\n",
            "   weighted avg       0.77      0.75      0.75       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[29  1  0  4  1]\n",
            " [ 9 18  0  3  0]\n",
            " [ 1  8 27  9  0]\n",
            " [ 0  5  2 38  2]\n",
            " [ 3  0  2  2 43]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.6917\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.78      0.74      0.76        42\n",
            "          tekno       0.46      0.50      0.48        46\n",
            "        warzone       0.68      0.82      0.74        44\n",
            "     industrial       0.69      0.57      0.62        42\n",
            "non-techno-drop       0.88      0.81      0.84        52\n",
            "\n",
            "       accuracy                           0.69       226\n",
            "      macro avg       0.69      0.69      0.69       226\n",
            "   weighted avg       0.70      0.69      0.69       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[31  5  0  4  2]\n",
            " [ 7 23  8  6  2]\n",
            " [ 0  7 36  1  0]\n",
            " [ 0  9  7 24  2]\n",
            " [ 2  6  2  0 42]]\n"
          ]
        }
      ],
      "source": [
        "### previous-ultimate model?! not performing so well anymore\n",
        "### >> focus on frequencies-bands rather than time/rhythm\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(1,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(len(CLASSES), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',  # for integer labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "historyD12 = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2          # prints: loss, acc, val_loss, val_acc each epoch\n",
        ")\n",
        "\n",
        "show_val_results(historyD12, X_val, y_val, CLASSES)\n",
        "show_test_results(model,   X_test, y_test, CLASSES)\n",
        "\n",
        "\n",
        "# low_melgrams\n",
        "# Best epoch training: accuracy 0.8538, loss 0.4033\n",
        "# Best epoch validation accuracy: 0.7921, loss 0.6513\n",
        "\n",
        "\n",
        "\n",
        "#################\n",
        "\n",
        "# results for normal melgrams , WITHOUT fixed tf.seed\n",
        "\n",
        "# Best epoch = 24\n",
        "#   train_acc 0.8314 | train_loss 0.4636\n",
        "#   val_acc   0.7585 | val_loss  0.7646\n",
        "# Weighted F1 (val) : 0.7536\n",
        "# Weighted F1 (test): 0.7448\n",
        "\n",
        "# Best epoch = 31\n",
        "#   train_acc 0.8956 | train_loss 0.3021\n",
        "#   val_acc   0.7681 | val_loss  0.7816\n",
        "# Weighted F1 (val) : 0.7725\n",
        "# Weighted F1 (test): 0.6985\n",
        "\n",
        "# Best epoch = 29\n",
        "#   train_acc 0.9312 | train_loss 0.2109\n",
        "#   val_acc   0.7488 | val_loss  0.8407\n",
        "# Weighted F1 (val) : 0.7499\n",
        "# Weighted F1 (test): 0.6917\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "QU4ESCxH4C9i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\me\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 - 9s - 654ms/step - accuracy: 0.2500 - loss: 1.6655 - val_accuracy: 0.3237 - val_loss: 1.5587\n",
            "Epoch 2/50\n",
            "14/14 - 8s - 552ms/step - accuracy: 0.3624 - loss: 1.4655 - val_accuracy: 0.4589 - val_loss: 1.2886\n",
            "Epoch 3/50\n",
            "14/14 - 7s - 496ms/step - accuracy: 0.4702 - loss: 1.2913 - val_accuracy: 0.4444 - val_loss: 1.2251\n",
            "Epoch 4/50\n",
            "14/14 - 7s - 486ms/step - accuracy: 0.5505 - loss: 1.1409 - val_accuracy: 0.5990 - val_loss: 1.0896\n",
            "Epoch 5/50\n",
            "14/14 - 8s - 580ms/step - accuracy: 0.6044 - loss: 1.0402 - val_accuracy: 0.6763 - val_loss: 0.9640\n",
            "Epoch 6/50\n",
            "14/14 - 9s - 623ms/step - accuracy: 0.6972 - loss: 0.8846 - val_accuracy: 0.6329 - val_loss: 0.8799\n",
            "Epoch 7/50\n",
            "14/14 - 7s - 531ms/step - accuracy: 0.7087 - loss: 0.7798 - val_accuracy: 0.7150 - val_loss: 0.7185\n",
            "Epoch 8/50\n",
            "14/14 - 7s - 497ms/step - accuracy: 0.7798 - loss: 0.6407 - val_accuracy: 0.7826 - val_loss: 0.7304\n",
            "Epoch 9/50\n",
            "14/14 - 7s - 520ms/step - accuracy: 0.7982 - loss: 0.5602 - val_accuracy: 0.7633 - val_loss: 0.5993\n",
            "Epoch 10/50\n",
            "14/14 - 7s - 523ms/step - accuracy: 0.8154 - loss: 0.5132 - val_accuracy: 0.8164 - val_loss: 0.5849\n",
            "Epoch 11/50\n",
            "14/14 - 8s - 538ms/step - accuracy: 0.8360 - loss: 0.4660 - val_accuracy: 0.7826 - val_loss: 0.5751\n",
            "Epoch 12/50\n",
            "14/14 - 7s - 504ms/step - accuracy: 0.8635 - loss: 0.4111 - val_accuracy: 0.8164 - val_loss: 0.5909\n",
            "Epoch 13/50\n",
            "14/14 - 8s - 593ms/step - accuracy: 0.9048 - loss: 0.3277 - val_accuracy: 0.8068 - val_loss: 0.5734\n",
            "Epoch 14/50\n",
            "14/14 - 7s - 514ms/step - accuracy: 0.9151 - loss: 0.2653 - val_accuracy: 0.8357 - val_loss: 0.6151\n",
            "Epoch 15/50\n",
            "14/14 - 7s - 503ms/step - accuracy: 0.8945 - loss: 0.2948 - val_accuracy: 0.7923 - val_loss: 0.6936\n",
            "Epoch 16/50\n",
            "14/14 - 7s - 517ms/step - accuracy: 0.8933 - loss: 0.3030 - val_accuracy: 0.7971 - val_loss: 0.5614\n",
            "Epoch 17/50\n",
            "14/14 - 7s - 493ms/step - accuracy: 0.9197 - loss: 0.2483 - val_accuracy: 0.7971 - val_loss: 0.5731\n",
            "Epoch 18/50\n",
            "14/14 - 7s - 518ms/step - accuracy: 0.9369 - loss: 0.1887 - val_accuracy: 0.8357 - val_loss: 0.5730\n",
            "Epoch 19/50\n",
            "14/14 - 7s - 514ms/step - accuracy: 0.9576 - loss: 0.1440 - val_accuracy: 0.8357 - val_loss: 0.6330\n",
            "Epoch 20/50\n",
            "14/14 - 7s - 501ms/step - accuracy: 0.9690 - loss: 0.1055 - val_accuracy: 0.8019 - val_loss: 0.6655\n",
            "Epoch 21/50\n",
            "14/14 - 7s - 486ms/step - accuracy: 0.9622 - loss: 0.1107 - val_accuracy: 0.8213 - val_loss: 0.6400\n",
            "Epoch 22/50\n",
            "14/14 - 7s - 488ms/step - accuracy: 0.9587 - loss: 0.1315 - val_accuracy: 0.7295 - val_loss: 0.7825\n",
            "Epoch 23/50\n",
            "14/14 - 7s - 471ms/step - accuracy: 0.9748 - loss: 0.0920 - val_accuracy: 0.8019 - val_loss: 0.6747\n",
            "Epoch 24/50\n",
            "14/14 - 7s - 483ms/step - accuracy: 0.9713 - loss: 0.0781 - val_accuracy: 0.8116 - val_loss: 0.8009\n",
            "Epoch 25/50\n",
            "14/14 - 7s - 509ms/step - accuracy: 0.9908 - loss: 0.0479 - val_accuracy: 0.8406 - val_loss: 0.7322\n",
            "Epoch 26/50\n",
            "14/14 - 7s - 485ms/step - accuracy: 0.9782 - loss: 0.0570 - val_accuracy: 0.7826 - val_loss: 0.9024\n",
            "\n",
            "Best epoch = 16\n",
            "  train_acc 0.8933 | train_loss 0.3030\n",
            "  val_acc   0.7971 | val_loss  0.5614\n",
            "\n",
            "Weighted F1 (val) : 0.7987\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.71      0.83      0.76        35\n",
            "          tekno       0.59      0.67      0.62        30\n",
            "        warzone       0.89      0.87      0.88        45\n",
            "     industrial       0.78      0.66      0.71        47\n",
            "non-techno-drop       0.96      0.92      0.94        50\n",
            "\n",
            "       accuracy                           0.80       207\n",
            "      macro avg       0.78      0.79      0.78       207\n",
            "   weighted avg       0.80      0.80      0.80       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[29  2  0  4  0]\n",
            " [ 5 20  2  2  1]\n",
            " [ 0  4 39  2  0]\n",
            " [ 7  6  2 31  1]\n",
            " [ 0  2  1  1 46]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.7395\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.77      0.86      0.81        42\n",
            "          tekno       0.60      0.76      0.67        46\n",
            "        warzone       0.69      0.77      0.73        44\n",
            "     industrial       0.68      0.50      0.58        42\n",
            "non-techno-drop       1.00      0.79      0.88        52\n",
            "\n",
            "       accuracy                           0.74       226\n",
            "      macro avg       0.75      0.74      0.73       226\n",
            "   weighted avg       0.76      0.74      0.74       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[36  3  0  3  0]\n",
            " [ 2 35  6  3  0]\n",
            " [ 0  7 34  3  0]\n",
            " [ 6 13  2 21  0]\n",
            " [ 3  0  7  1 41]]\n"
          ]
        }
      ],
      "source": [
        "### rebuilding the best model so far\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "final_model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,1), padding='same'),\n",
        "    MaxPooling2D(2,2), # MAYBE (2,1) OR (1,2) ?\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels_list), activation='softmax')\n",
        "])\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "final_history = final_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2          # prints: loss, acc, val_loss, val_acc each epoch\n",
        ")\n",
        "\n",
        "show_val_results(final_history, X_val, y_val, CLASSES)\n",
        "show_test_results(final_model,   X_test, y_test, CLASSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "_5yt8Lf4xdSu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as data/genres_transfer_model.h5\n"
          ]
        }
      ],
      "source": [
        "## saving the final model\n",
        "\n",
        "\n",
        "teliko_modelo = \"data/genres_transfer_model.h5\"\n",
        "final_model.save(teliko_modelo)\n",
        "print(f\"Model saved as {teliko_modelo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ePlJDxXHxfac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "  X_train: (699, 128, 128, 1), y_train: (699,)\n",
            "  X_val:   (150, 128, 128, 1),   y_val:   (150,)\n",
            "  X_test:  (150, 128, 128, 1),  y_test:  (150,)\n"
          ]
        }
      ],
      "source": [
        "### prep-split for transfer learning\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "DATA_DIR = \"data/transfer_set\"\n",
        "IMAGE_SIZE = (128, 128)\n",
        "SEED = 42\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# list of genre folders\n",
        "genres = sorted(os.listdir(DATA_DIR))\n",
        "genre_to_idx = {genre: idx for idx, genre in enumerate(genres)}\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for genre in genres:\n",
        "    genre_folder = os.path.join(DATA_DIR, genre)\n",
        "    for fname in os.listdir(genre_folder):\n",
        "        if fname.lower().endswith('.png'):\n",
        "            img_path = os.path.join(genre_folder, fname)\n",
        "\n",
        "            # grayscale AGAIN\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, IMAGE_SIZE)\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "            X.append(img)\n",
        "            y.append(genre_to_idx[genre])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# split into train 70%, temp 30% >> then split temp into val & test\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    random_state=SEED,\n",
        "    stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=SEED,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"  X_val:   {X_val.shape},   y_val:   {y_val.shape}\")\n",
        "print(f\"  X_test:  {X_test.shape},  y_test:  {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HlLxmtAJxjzs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - accuracy: 0.0878 - loss: 4.6430 - val_accuracy: 0.1000 - val_loss: 2.2983\n",
            "Epoch 2/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.1076 - loss: 2.2906 - val_accuracy: 0.2200 - val_loss: 2.2432\n",
            "Epoch 3/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 279ms/step - accuracy: 0.1880 - loss: 2.2437 - val_accuracy: 0.3333 - val_loss: 2.0886\n",
            "Epoch 4/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 295ms/step - accuracy: 0.2717 - loss: 2.0895 - val_accuracy: 0.4200 - val_loss: 1.8731\n",
            "Epoch 5/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.3120 - loss: 1.9384 - val_accuracy: 0.4333 - val_loss: 1.7306\n",
            "Epoch 6/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 285ms/step - accuracy: 0.3475 - loss: 1.8476 - val_accuracy: 0.4600 - val_loss: 1.6125\n",
            "Epoch 7/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 288ms/step - accuracy: 0.3819 - loss: 1.7440 - val_accuracy: 0.5267 - val_loss: 1.4797\n",
            "Epoch 8/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 282ms/step - accuracy: 0.4234 - loss: 1.6546 - val_accuracy: 0.4400 - val_loss: 1.4676\n",
            "Epoch 9/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 303ms/step - accuracy: 0.4174 - loss: 1.5465 - val_accuracy: 0.5867 - val_loss: 1.2933\n",
            "Epoch 10/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 304ms/step - accuracy: 0.4898 - loss: 1.4650 - val_accuracy: 0.4400 - val_loss: 1.4353\n",
            "Epoch 11/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 279ms/step - accuracy: 0.4978 - loss: 1.4489 - val_accuracy: 0.5933 - val_loss: 1.2432\n",
            "Epoch 12/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333ms/step - accuracy: 0.5582 - loss: 1.3117 - val_accuracy: 0.6200 - val_loss: 1.2198\n",
            "Epoch 13/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 289ms/step - accuracy: 0.5642 - loss: 1.2493 - val_accuracy: 0.5267 - val_loss: 1.1860\n",
            "Epoch 14/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 286ms/step - accuracy: 0.6218 - loss: 1.1111 - val_accuracy: 0.5333 - val_loss: 1.2384\n",
            "Epoch 15/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 302ms/step - accuracy: 0.5938 - loss: 1.1289 - val_accuracy: 0.5867 - val_loss: 1.1065\n",
            "Epoch 16/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 321ms/step - accuracy: 0.6137 - loss: 1.0703 - val_accuracy: 0.6333 - val_loss: 1.0715\n",
            "Epoch 17/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 311ms/step - accuracy: 0.6657 - loss: 0.9806 - val_accuracy: 0.6800 - val_loss: 1.0591\n",
            "Epoch 18/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 306ms/step - accuracy: 0.6734 - loss: 0.9510 - val_accuracy: 0.6333 - val_loss: 1.0296\n",
            "Epoch 19/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 326ms/step - accuracy: 0.6889 - loss: 0.7932 - val_accuracy: 0.6667 - val_loss: 0.9886\n",
            "Epoch 20/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 306ms/step - accuracy: 0.7299 - loss: 0.7556 - val_accuracy: 0.6533 - val_loss: 1.0266\n",
            "Epoch 21/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 326ms/step - accuracy: 0.7317 - loss: 0.7148 - val_accuracy: 0.6933 - val_loss: 0.9643\n",
            "Epoch 22/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 304ms/step - accuracy: 0.7759 - loss: 0.6572 - val_accuracy: 0.6867 - val_loss: 0.9638\n",
            "Epoch 23/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 305ms/step - accuracy: 0.8190 - loss: 0.5424 - val_accuracy: 0.6467 - val_loss: 1.0183\n",
            "Epoch 24/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 302ms/step - accuracy: 0.8015 - loss: 0.5465 - val_accuracy: 0.6600 - val_loss: 1.1001\n",
            "Epoch 25/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 303ms/step - accuracy: 0.8069 - loss: 0.5464 - val_accuracy: 0.6333 - val_loss: 1.1509\n",
            "Epoch 26/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 301ms/step - accuracy: 0.8073 - loss: 0.5489 - val_accuracy: 0.6800 - val_loss: 1.0618\n",
            "Epoch 27/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 301ms/step - accuracy: 0.8319 - loss: 0.4532 - val_accuracy: 0.6733 - val_loss: 1.0494\n",
            "Epoch 28/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 304ms/step - accuracy: 0.8730 - loss: 0.3829 - val_accuracy: 0.6533 - val_loss: 1.1383\n",
            "Epoch 29/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 302ms/step - accuracy: 0.9156 - loss: 0.3178 - val_accuracy: 0.6800 - val_loss: 1.0943\n",
            "Epoch 30/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 306ms/step - accuracy: 0.8920 - loss: 0.3124 - val_accuracy: 0.7067 - val_loss: 1.1541\n",
            "Epoch 31/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 305ms/step - accuracy: 0.9053 - loss: 0.2699 - val_accuracy: 0.6667 - val_loss: 1.2096\n",
            "Epoch 32/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 299ms/step - accuracy: 0.9264 - loss: 0.2442 - val_accuracy: 0.6733 - val_loss: 1.2209\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1cc51a6cc70>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### CNN for transfer learning\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# the original model\n",
        "base_model = load_model(teliko_modelo, compile=False)\n",
        "\n",
        "# the output of the penultimate layer\n",
        "x = base_model.layers[-2].output\n",
        "\n",
        "# new output layer for 10 genres\n",
        "new_output = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create new model\n",
        "transfer_model = Model(inputs=base_model.layers[0].input, outputs=new_output)\n",
        "\n",
        "# freezing earlier layers\n",
        "for layer in transfer_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "transfer_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# training on the 10-genre dataset\n",
        "transfer_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "q-DxBkSP5qQe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.6193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.57      0.53      0.55        15\n",
            "   classical       0.87      0.87      0.87        15\n",
            "     country       0.44      0.80      0.57        15\n",
            "       disco       0.50      0.53      0.52        15\n",
            "      hiphop       0.50      0.27      0.35        15\n",
            "        jazz       0.75      0.80      0.77        15\n",
            "       metal       0.92      0.73      0.81        15\n",
            "         pop       0.71      0.80      0.75        15\n",
            "      reggae       0.67      0.80      0.73        15\n",
            "        rock       0.43      0.20      0.27        15\n",
            "\n",
            "    accuracy                           0.63       150\n",
            "   macro avg       0.64      0.63      0.62       150\n",
            "weighted avg       0.64      0.63      0.62       150\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[ 8  0  4  1  0  2  0  0  0  0]\n",
            " [ 0 13  1  0  0  0  0  1  0  0]\n",
            " [ 1  0 12  0  0  1  0  0  0  1]\n",
            " [ 0  1  2  8  3  0  0  1  0  0]\n",
            " [ 0  0  0  5  4  0  0  2  4  0]\n",
            " [ 2  0  1  0  0 12  0  0  0  0]\n",
            " [ 1  0  0  0  1  0 11  0  0  2]\n",
            " [ 0  0  1  1  0  0  0 12  0  1]\n",
            " [ 1  0  1  0  0  0  0  1 12  0]\n",
            " [ 1  1  5  1  0  1  1  0  2  3]]\n"
          ]
        }
      ],
      "source": [
        "genres = sorted(os.listdir(DATA_DIR))\n",
        "genre_to_idx = {g:i for i,g in enumerate(genres)}\n",
        "trans_class_names = genres\n",
        "\n",
        "show_test_results(transfer_model, X_test, y_test, trans_class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "76pjqr-050fX"
      },
      "outputs": [],
      "source": [
        "### RESULTS for the test's f1-score after 5 consecutive runs for *tranfer learning*\n",
        "\n",
        "\n",
        "# run 1\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6343\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.67      0.67      0.67        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.42      0.73      0.54        15\n",
        "#        disco       0.67      0.53      0.59        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.63      0.80      0.71        15\n",
        "#        metal       0.80      0.80      0.80        15\n",
        "#          pop       0.69      0.60      0.64        15\n",
        "#       reggae       0.73      0.53      0.62        15\n",
        "#         rock       0.57      0.27      0.36        15\n",
        "\n",
        "#     accuracy                           0.64       150\n",
        "#    macro avg       0.66      0.64      0.63       150\n",
        "# weighted avg       0.66      0.64      0.63       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[10  0  3  0  0  2  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 0  1 11  0  0  2  0  0  0  1]\n",
        "#  [ 0  1  2  8  4  0  0  0  0  0]\n",
        "#  [ 0  0  0  1  9  0  1  2  2  0]\n",
        "#  [ 2  0  1  0  0 12  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 12  0  0  1]\n",
        "#  [ 0  0  1  3  1  0  0  9  0  1]\n",
        "#  [ 1  0  2  0  1  1  1  1  8  0]\n",
        "#  [ 1  1  5  0  0  2  1  0  1  4]]\n",
        "\n",
        "\n",
        "# run 2\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6247\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.59      0.67      0.62        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.44      0.53      0.48        15\n",
        "#        disco       0.40      0.53      0.46        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.85      0.73      0.79        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.64      0.60      0.62        15\n",
        "#       reggae       0.89      0.53      0.67        15\n",
        "#         rock       0.46      0.40      0.43        15\n",
        "\n",
        "#     accuracy                           0.62       150\n",
        "#    macro avg       0.64      0.62      0.62       150\n",
        "# weighted avg       0.64      0.62      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[10  0  3  2  0  0  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 0  1  8  0  0  2  0  0  0  4]\n",
        "#  [ 0  1  1  8  4  0  0  1  0  0]\n",
        "#  [ 0  0  0  3  9  0  1  2  0  0]\n",
        "#  [ 3  0  1  0  0 11  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 11  0  0  2]\n",
        "#  [ 0  0  1  4  0  0  0  9  0  1]\n",
        "#  [ 1  0  0  2  2  0  1  1  8  0]\n",
        "#  [ 2  1  3  1  0  0  1  0  1  6]]\n",
        "\n",
        "# run 3\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6327\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.52      0.80      0.63        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.64      0.60      0.62        15\n",
        "#        disco       0.53      0.53      0.53        15\n",
        "#       hiphop       0.57      0.53      0.55        15\n",
        "#         jazz       0.62      0.87      0.72        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.67      0.53      0.59        15\n",
        "#       reggae       0.69      0.60      0.64        15\n",
        "#         rock       0.62      0.33      0.43        15\n",
        "\n",
        "#     accuracy                           0.64       150\n",
        "#    macro avg       0.65      0.64      0.63       150\n",
        "# weighted avg       0.65      0.64      0.63       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[12  0  0  0  0  3  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 1  1  9  0  0  3  0  0  0  1]\n",
        "#  [ 1  1  1  8  4  0  0  0  0  0]\n",
        "#  [ 0  0  0  2  8  0  1  2  2  0]\n",
        "#  [ 2  0  0  0  0 13  0  0  0  0]\n",
        "#  [ 2  0  0  0  1  0 11  0  0  1]\n",
        "#  [ 0  0  1  5  0  0  0  8  0  1]\n",
        "#  [ 2  0  0  0  1  1  1  1  9  0]\n",
        "#  [ 3  1  2  0  0  1  1  0  2  5]]\n",
        "\n",
        "# run 4\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6214\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.57      0.53      0.55        15\n",
        "#    classical       0.82      0.93      0.88        15\n",
        "#      country       0.45      0.60      0.51        15\n",
        "#        disco       0.54      0.47      0.50        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.71      0.80      0.75        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.67      0.67      0.67        15\n",
        "#       reggae       0.53      0.60      0.56        15\n",
        "#         rock       0.71      0.33      0.45        15\n",
        "\n",
        "#     accuracy                           0.63       150\n",
        "#    macro avg       0.63      0.63      0.62       150\n",
        "# weighted avg       0.63      0.63      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[ 8  0  3  0  0  1  0  0  3  0]\n",
        "#  [ 0 14  0  0  0  0  0  1  0  0]\n",
        "#  [ 0  1  9  1  0  3  0  0  0  1]\n",
        "#  [ 0  1  2  7  3  0  0  1  1  0]\n",
        "#  [ 0  0  0  1  9  0  1  2  2  0]\n",
        "#  [ 3  0  0  0  0 12  0  0  0  0]\n",
        "#  [ 1  0  1  1  1  0 11  0  0  0]\n",
        "#  [ 0  0  1  2  1  0  0 10  0  1]\n",
        "#  [ 1  0  1  0  2  0  1  1  9  0]\n",
        "#  [ 1  1  3  1  0  1  1  0  2  5]]\n",
        "\n",
        "# run 5\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6196\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.67      0.53      0.59        15\n",
        "#    classical       0.74      0.93      0.82        15\n",
        "#      country       0.64      0.47      0.54        15\n",
        "#        disco       0.44      0.47      0.45        15\n",
        "#       hiphop       0.53      0.60      0.56        15\n",
        "#         jazz       0.67      0.93      0.78        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.53      0.60      0.56        15\n",
        "#       reggae       0.75      0.60      0.67        15\n",
        "#         rock       0.55      0.40      0.46        15\n",
        "\n",
        "#     accuracy                           0.63       150\n",
        "#    macro avg       0.63      0.63      0.62       150\n",
        "# weighted avg       0.63      0.63      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[ 8  0  1  2  0  3  0  0  1  0]\n",
        "#  [ 0 14  0  0  0  0  0  1  0  0]\n",
        "#  [ 0  2  7  1  0  2  0  1  0  2]\n",
        "#  [ 0  1  1  7  4  0  0  2  0  0]\n",
        "#  [ 0  0  0  3  9  0  1  2  0  0]\n",
        "#  [ 1  0  0  0  0 14  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 11  0  0  2]\n",
        "#  [ 0  0  1  3  1  0  0  9  0  1]\n",
        "#  [ 1  0  0  0  2  0  1  2  9  0]\n",
        "#  [ 1  2  1  0  0  2  1  0  2  6]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "AbiLzFejxpCq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.9342, Validation Accuracy: 0.6867\n",
            "Class distribution in Training Set:\n",
            "  Class 0: 70 samples, 10.01%\n",
            "  Class 1: 70 samples, 10.01%\n",
            "  Class 2: 70 samples, 10.01%\n",
            "  Class 3: 70 samples, 10.01%\n",
            "  Class 4: 70 samples, 10.01%\n",
            "  Class 5: 69 samples, 9.87%\n",
            "  Class 6: 70 samples, 10.01%\n",
            "  Class 7: 70 samples, 10.01%\n",
            "  Class 8: 70 samples, 10.01%\n",
            "  Class 9: 70 samples, 10.01%\n",
            "  Total samples: 699\n",
            "\n",
            "Class distribution in Validation Set:\n",
            "  Class 0: 15 samples, 10.00%\n",
            "  Class 1: 15 samples, 10.00%\n",
            "  Class 2: 15 samples, 10.00%\n",
            "  Class 3: 15 samples, 10.00%\n",
            "  Class 4: 15 samples, 10.00%\n",
            "  Class 5: 15 samples, 10.00%\n",
            "  Class 6: 15 samples, 10.00%\n",
            "  Class 7: 15 samples, 10.00%\n",
            "  Class 8: 15 samples, 10.00%\n",
            "  Class 9: 15 samples, 10.00%\n",
            "  Total samples: 150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#printing the accuracy and val accuracy of the final weights\n",
        "train_loss, train_acc = transfer_model.evaluate(X_train, y_train, verbose=0)\n",
        "val_loss, val_acc = transfer_model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# printing the per class distribution in x and y\n",
        "from collections import Counter\n",
        "def print_class_distribution(y, split_name):\n",
        "    counts = Counter(y)\n",
        "    total = len(y)\n",
        "    print(f\"Class distribution in {split_name}:\")\n",
        "\n",
        "    for cls, count in sorted(counts.items()):\n",
        "        print(f\"  Class {cls}: {count} samples, {count / total:.2%}\")\n",
        "    print(f\"  Total samples: {total}\\n\")\n",
        "print_class_distribution(y_train, 'Training Set')\n",
        "print_class_distribution(y_val, 'Validation Set')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
