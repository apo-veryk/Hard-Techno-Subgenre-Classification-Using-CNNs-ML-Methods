{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCrTj-er5Y7m"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/MyDrive/MSc/deep_learning\"\n",
        "# !dir \"data/melgrams\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGr9Z7o9nTFg"
      },
      "outputs": [],
      "source": [
        "#setting input files and folder-dirs\n",
        "\n",
        "\n",
        "\n",
        "CLASSES = ['bouncy','tekno','warzone','industrial','non-techno-drop']\n",
        "# REMOVED: 'psytech',\n",
        "\n",
        "# csv_file_path = \"/content/drive/MyDrive/MSc/deep_learning/labels_final.csv\"\n",
        "# tracks_dir = \"/content/drive/MyDrive/MSc/deep_learning/tracks_final\"\n",
        "# clips_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/clips\"\n",
        "\n",
        "# norm_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/norm_melgrams_dir\"\n",
        "# low_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/low_melgrams\"\n",
        "# melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "\n",
        "csv_file_path = \"data/labels_final.csv\"\n",
        "tracks_dir = \"data/tracks_final\"\n",
        "clips_dir = \"data/clips\"\n",
        "\n",
        "norm_melgrams_dir = \"data/melgrams\"\n",
        "low_melgrams_dir = \"data/low_melgrams\"\n",
        "melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "import os\n",
        "os.makedirs(clips_dir, exist_ok=True)\n",
        "os.makedirs(norm_melgrams_dir, exist_ok=True)\n",
        "os.makedirs(low_melgrams_dir, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# import random, numpy as np\n",
        "# from tensorflow.keras.utils import set_random_seed\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# tf.random.set_seed(SEED)\n",
        "# set_random_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F6T-Mljy-TW9"
      },
      "outputs": [],
      "source": [
        "# # sample-clipper > clips are within \"deep_learning/clips\" (including non-techno-drop)\n",
        "\n",
        "\n",
        "\n",
        "# import os, pandas as pd, librosa, soundfile as sf\n",
        "\n",
        "# df = pd.read_csv(csv_file_path)\n",
        "# total_clips = len(df)\n",
        "# processed = 0\n",
        "\n",
        "# # convert m:ss to float seconds\n",
        "\n",
        "# def ts_to_sec(ts: str) -> float:\n",
        "#     if \":\" in ts:\n",
        "#         m, s = map(int, ts.split(\":\"))\n",
        "#         return m * 60 + s\n",
        "#     else:\n",
        "#         return float(ts)\n",
        "\n",
        "# # iterating rows and create clips\n",
        "\n",
        "# for idx, row in df.iterrows():\n",
        "#     spot_id    = str(row[\"spot_id\"])\n",
        "#     sample_id  = str(row[\"sample_id\"])\n",
        "#     clip_start = ts_to_sec(str(row[\"clip_start\"]))\n",
        "\n",
        "#     # skip if clip exists\n",
        "#     dst = os.path.join(clips_dir, f\"{sample_id}.mp3\")\n",
        "#     if os.path.exists(dst):\n",
        "#         print(f\"[SKIP] Clip exists: {sample_id}.mp3\")\n",
        "#         processed += 1\n",
        "#         continue\n",
        "\n",
        "#     duration = 8\n",
        "\n",
        "#     src = os.path.join(tracks_dir, f\"{spot_id}.mp3\")\n",
        "\n",
        "#     if not os.path.exists(src):\n",
        "#         print(f\"[SKIP] track file not found: {src}\")\n",
        "#         processed += 1\n",
        "#         continue\n",
        "\n",
        "#     try:\n",
        "#         y, sr = librosa.load(src, sr=None)\n",
        "#         beg = int(clip_start * sr)\n",
        "#         end = int((clip_start + duration) * sr)\n",
        "#         clip = y[beg:end]\n",
        "\n",
        "#         sf.write(dst, clip, sr)\n",
        "#         processed += 1\n",
        "#         print(f\"[OK] {sample_id}.mp3 | {duration}s ({processed}/{total_clips})\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"[ERR] {sample_id}: {e}\")\n",
        "\n",
        "# print(f\"done {processed}/{total_clips} clips.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiUZYUL3dCr9"
      },
      "outputs": [],
      "source": [
        "### see how many we clips we got after all\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "n_written = sum(1 for f in os.listdir(clips_dir))\n",
        "print(n_written)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hd3dBypA0Gh",
        "outputId": "59e73072-5ca9-4b98-b767-06ba4dbeb5a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n"
          ]
        }
      ],
      "source": [
        "### features for ML predictions with librosa\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# function to get features with librosa\n",
        "def extract_features(audio_file):\n",
        "    y, sr = librosa.load(audio_file)\n",
        "\n",
        "    # feature list\n",
        "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    rmse = librosa.feature.rms(y=y)\n",
        "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "\n",
        "    # getting means\n",
        "    return [\n",
        "        np.mean(chroma_stft),\n",
        "        np.mean(rmse),\n",
        "        np.mean(spec_cent),\n",
        "        np.mean(spec_bw),\n",
        "        np.mean(rolloff),\n",
        "        np.mean(zcr),\n",
        "        np.mean(mfcc[0]),\n",
        "        np.mean(mfcc[1]),\n",
        "        np.mean(mfcc[2]),\n",
        "        np.mean(mfcc[3]),\n",
        "        np.mean(mfcc[4]),\n",
        "        np.mean(mfcc[5]),\n",
        "        np.mean(mfcc[6]),\n",
        "\n",
        "    ]\n",
        "\n",
        "# going through the clips\n",
        "for filename in os.listdir(clips_dir):\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        clip_path = os.path.join(clips_dir, filename)\n",
        "\n",
        "        # getting features\n",
        "        clip_features = extract_features(clip_path)\n",
        "        features.append(clip_features)\n",
        "\n",
        "        # getting the name as well, to match them later\n",
        "        sample_id = filename[:-4]\n",
        "\n",
        "        # matching with the labels from the csv\n",
        "        try:\n",
        "            row = df_csv.loc[df_csv['sample_id'] == sample_id, CLASSES].iloc[0].tolist()\n",
        "            # convert duration → binary flag (misc)\n",
        "            misc_val   = row[-1]\n",
        "            misc_label = 1 if pd.notna(misc_val) and int(misc_val) > 0 else 0\n",
        "            row[-1]    = misc_label\n",
        "\n",
        "        except IndexError:\n",
        "            print(f\"[WARN] no label for {sample_id}\")\n",
        "            row = [np.nan]*8\n",
        "\n",
        "        labels.append(row)\n",
        "\n",
        "\n",
        "# now we will get it all into a dataframe to work with it in the svm\n",
        "feature_names = [\n",
        "    'chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'\n",
        "]\n",
        "df_features = pd.DataFrame(features, columns=feature_names)\n",
        "df_labels = pd.DataFrame(labels, columns=CLASSES)\n",
        "df_final = pd.concat([df_features, df_labels], axis=1)\n",
        "\n",
        "# saving it\n",
        "df_final.to_csv(\"data/clip_features_and_labels_2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZK_aEIiU6KHu"
      },
      "outputs": [],
      "source": [
        "# !pip install pyAudioAnalysis eyed3 pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArIGzrTn5DkJ"
      },
      "outputs": [],
      "source": [
        "### features for ML predictions with pyAudioAnalysis\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from pyAudioAnalysis.audioBasicIO import read_audio_file\n",
        "from pyAudioAnalysis.ShortTermFeatures import feature_extraction\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# extract features with pyAudioAnalysis\n",
        "def extract_features(audio_file):\n",
        "    try:\n",
        "        [fs, x] = read_audio_file(audio_file)\n",
        "\n",
        "        # extracting features\n",
        "        [feats, _] = feature_extraction(\n",
        "            x, fs, 0.050 * fs, 0.025 * fs)\n",
        "\n",
        "        # feature mapping as before\n",
        "        return [\n",
        "            np.mean(feats[13:25]),\n",
        "            np.mean(feats[1]),\n",
        "            np.mean(feats[3]),\n",
        "            np.mean(feats[4]),\n",
        "            np.mean(feats[7]),\n",
        "            np.mean(feats[0]),\n",
        "            *[np.mean(feats[8+i]) for i in range(5)],\n",
        "            np.mean(feats[8:13]),\n",
        "            np.mean(feats[8:13])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "for filename in os.listdir(clips_dir):\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        clip_path = os.path.join(clips_dir, filename)\n",
        "\n",
        "        # get the features\n",
        "        try:\n",
        "            clip_features = extract_features(clip_path)\n",
        "            features.append(clip_features)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # match labels from csv\n",
        "        sample_id = filename[:-4]\n",
        "        try:\n",
        "            clip_labels = df_csv.loc[\n",
        "                df_csv['sample_id'] == sample_id, CLASSES].values.tolist()[0]\n",
        "        except IndexError:\n",
        "            print(f\"Warning: No label found for sample_id: {sample_id}\")\n",
        "            clip_labels = [np.nan] * 8\n",
        "\n",
        "        labels.append(clip_labels)\n",
        "\n",
        "# creating the dataframes\n",
        "feature_names = [\n",
        "    'chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'\n",
        "]\n",
        "df_features = pd.DataFrame(features, columns=feature_names)\n",
        "df_labels = pd.DataFrame(labels, columns=CLASSES)\n",
        "df_final = pd.concat([df_features, df_labels], axis=1)\n",
        "\n",
        "# saving\n",
        "df_final.to_csv(\"data/clip_features_and_labels_pyAudioAnalysis.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1soqjxyp7ElX"
      },
      "outputs": [],
      "source": [
        "### filtering rows\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# loading the df from before\n",
        "df = pd.read_csv(\"data/clip_features_and_labels_2.csv\") # << Librosa\n",
        "# df = pd.read_csv(\"data/clip_features_and_labels_pyAudioAnalysis.csv\") # << pyAudioAnalysis\n",
        "\n",
        "df['label_sum'] = df[CLASSES].sum(axis=1)\n",
        "filtered_df = df[df['label_sum'] >= 1]\n",
        "filtered_df = filtered_df.drop(columns=['label_sum'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5T1PqpA7HZv"
      },
      "outputs": [],
      "source": [
        "### prep-split & predictions based on features\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# features and labels\n",
        "X = filtered_df[['chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7']]\n",
        "y = filtered_df[CLASSES]\n",
        "\n",
        "# splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# scaling for svm\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxlIqR-N7dpu",
        "outputId": "ac6caa89-2c51-4a1d-d278-44d7202561b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in labels (y_train):\n",
            " bouncy             0\n",
            "tekno              0\n",
            "warzone            0\n",
            "industrial         0\n",
            "non-techno-drop    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in features (X_train):\n",
            " chroma_stft    0\n",
            "rmse           0\n",
            "spec_cent      0\n",
            "spec_bw        0\n",
            "rolloff        0\n",
            "zcr            0\n",
            "mfcc1          0\n",
            "mfcc2          0\n",
            "mfcc3          0\n",
            "mfcc4          0\n",
            "mfcc5          0\n",
            "mfcc6          0\n",
            "mfcc7          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# purely for debugging during testing\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# checking for missing values\n",
        "missing_labels = y_train.isnull().sum()\n",
        "print(\"Missing values in labels (y_train):\\n\", missing_labels)\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train, columns=['chroma_stft', 'rmse', 'spec_cent', 'spec_bw', 'rolloff', 'zcr', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7'])\n",
        "missing_features = X_train_df.isnull().sum()\n",
        "print(\"\\nMissing values in features (X_train):\\n\", missing_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8tuJroeF0MS",
        "outputId": "996b750d-2159-44e0-a288-aa2c3d4f79c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics for bouncy:\n",
            "Accuracy:  0.8544061302681992\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.86      0.91       213\n",
            "           1       0.57      0.83      0.68        48\n",
            "\n",
            "    accuracy                           0.85       261\n",
            "   macro avg       0.76      0.85      0.79       261\n",
            "weighted avg       0.89      0.85      0.86       261\n",
            "\n",
            "\n",
            "Metrics for tekno:\n",
            "Accuracy:  0.8275862068965517\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.84      0.89       208\n",
            "           1       0.55      0.79      0.65        53\n",
            "\n",
            "    accuracy                           0.83       261\n",
            "   macro avg       0.75      0.81      0.77       261\n",
            "weighted avg       0.86      0.83      0.84       261\n",
            "\n",
            "\n",
            "Metrics for warzone:\n",
            "Accuracy:  0.8390804597701149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.84      0.89       209\n",
            "           1       0.56      0.85      0.68        52\n",
            "\n",
            "    accuracy                           0.84       261\n",
            "   macro avg       0.76      0.84      0.78       261\n",
            "weighted avg       0.88      0.84      0.85       261\n",
            "\n",
            "\n",
            "Metrics for industrial:\n",
            "Accuracy:  0.789272030651341\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.78      0.85       200\n",
            "           1       0.53      0.84      0.65        61\n",
            "\n",
            "    accuracy                           0.79       261\n",
            "   macro avg       0.74      0.81      0.75       261\n",
            "weighted avg       0.84      0.79      0.80       261\n",
            "\n",
            "\n",
            "Metrics for non-techno-drop:\n",
            "Accuracy:  0.9808429118773946\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       214\n",
            "           1       0.90      1.00      0.95        47\n",
            "\n",
            "    accuracy                           0.98       261\n",
            "   macro avg       0.95      0.99      0.97       261\n",
            "weighted avg       0.98      0.98      0.98       261\n",
            "\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.57      0.83      0.68        48\n",
            "          tekno       0.55      0.79      0.65        53\n",
            "        warzone       0.56      0.85      0.68        52\n",
            "     industrial       0.53      0.84      0.65        61\n",
            "non-techno-drop       0.90      1.00      0.95        47\n",
            "\n",
            "      micro avg       0.60      0.86      0.71       261\n",
            "      macro avg       0.62      0.86      0.72       261\n",
            "   weighted avg       0.62      0.86      0.71       261\n",
            "    samples avg       0.68      0.86      0.74       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new SVM for single-label\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "svm = OneVsRestClassifier(\n",
        "        SVC(kernel='rbf', probability=False, class_weight='balanced'))\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "for i, label in enumerate(y.columns):\n",
        "    print(f\"\\nMetrics for {label}:\")\n",
        "    print(\"Accuracy: \", accuracy_score(y_test.iloc[:, i], y_pred[:, i]))\n",
        "    print(classification_report(y_test.iloc[:, i], y_pred[:, i]))\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=CLASSES))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P6wICD7CEp3",
        "outputId": "4cfeaeee-829b-4ef1-e13d-0cb986c10a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.47081\tval-mlogloss:1.50340\n",
            "[50]\ttrain-mlogloss:0.17770\tval-mlogloss:0.71909\n",
            "[100]\ttrain-mlogloss:0.06269\tval-mlogloss:0.71652\n",
            "\n",
            "=== Overall report on XGBoost validation set ===\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.63      0.69      0.66        48\n",
            "          tekno       0.68      0.68      0.68        53\n",
            "        warzone       0.73      0.79      0.76        52\n",
            "     industrial       0.76      0.64      0.70        61\n",
            "non-techno-drop       0.92      0.96      0.94        47\n",
            "\n",
            "       accuracy                           0.74       261\n",
            "      macro avg       0.75      0.75      0.75       261\n",
            "   weighted avg       0.74      0.74      0.74       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new XGBoost for single-label\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd, xgboost as xgb, numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "Dy_train = np.asarray(y_train)\n",
        "Dy_test  = np.asarray(y_test)\n",
        "\n",
        "Dy_train_idx = np.argmax(Dy_train, axis=1)\n",
        "Dy_test_idx  = np.argmax(Dy_test,  axis=1)\n",
        "\n",
        "num_class = Dy_train.shape[1]\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=Dy_train_idx)\n",
        "dval   = xgb.DMatrix(X_test,  label=Dy_test_idx)\n",
        "\n",
        "params = dict(\n",
        "    objective     = 'multi:softprob',\n",
        "    num_class     = num_class,\n",
        "    eval_metric   = 'mlogloss',\n",
        "    max_depth     = 6,\n",
        "    eta           = 0.1,\n",
        "    subsample     = 0.9,\n",
        "    colsample_bytree = 0.9\n",
        ")\n",
        "\n",
        "booster = xgb.train(params, dtrain,\n",
        "                    num_boost_round=600,\n",
        "                    evals=[(dtrain,'train'), (dval,'val')],\n",
        "                    early_stopping_rounds=30,\n",
        "                    verbose_eval=50)\n",
        "\n",
        "\n",
        "y_pred = booster.predict(dval).argmax(axis=1)\n",
        "\n",
        "#  overall multi class report\n",
        "print(\"\\n=== Overall report on XGBoost validation set ===\")\n",
        "print(classification_report(Dy_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ZeRPbZVs-b",
        "outputId": "1ec51b47-302a-4255-b3f9-2e708952a172"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:46:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGB  macro-F1: 0.7675351811960717 \n",
            "Params: {'tree_method': 'hist', 'subsample': 0.85, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 8, 'lambda_l2': 1.0, 'lambda_l1': 1.0, 'gamma': 0.5, 'eta': 0.03, 'colsample_bytree': 0.6}\n"
          ]
        }
      ],
      "source": [
        "### hyper-tunning XGBoost\n",
        "\n",
        "\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'max_depth'        : [4, 6, 8, 10],\n",
        "    'eta'              : [0.30, 0.15, 0.07, 0.03],\n",
        "    'n_estimators'     : [200, 400, 800, 1200],\n",
        "    'subsample'        : [0.7, 0.85, 1.0],\n",
        "    'colsample_bytree' : [0.6, 0.8, 1.0],\n",
        "    'min_child_weight' : [1, 3, 5],\n",
        "    'gamma'            : [0, 0.5, 1],\n",
        "    'lambda_l1'        : [0,  1.0],\n",
        "    'lambda_l2'        : [0,  1.0],\n",
        "    'tree_method'      : ['hist'],\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.metrics import f1_score\n",
        "search = list(ParameterSampler(param_grid_xgb, n_iter=25, random_state=SEED))\n",
        "\n",
        "best_macro_f1 = 0\n",
        "best_params   = None\n",
        "\n",
        "for p in search:\n",
        "    model = xgb.XGBClassifier(objective='multi:softprob',\n",
        "                              num_class=len(CLASSES),\n",
        "                              eval_metric='mlogloss',\n",
        "                              early_stopping_rounds=50,\n",
        "                              **p)\n",
        "    model.fit(X_train, Dy_train_idx,\n",
        "              eval_set=[(X_test, Dy_test_idx)],\n",
        "              verbose=False)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(Dy_test_idx, y_pred, average='macro')\n",
        "    if f1 > best_macro_f1:\n",
        "        best_macro_f1, best_params = f1, p\n",
        "\n",
        "print(\"Best XGB  macro-F1:\", best_macro_f1, \"\\nParams:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YC_7yKXXL86",
        "outputId": "7f704bed-e04b-4bf5-9d08-af9d6147e3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.56760\tval-mlogloss:1.57687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [06:47:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50]\ttrain-mlogloss:0.61683\tval-mlogloss:0.94030\n",
            "[100]\ttrain-mlogloss:0.33241\tval-mlogloss:0.76317\n",
            "[150]\ttrain-mlogloss:0.22592\tval-mlogloss:0.71270\n",
            "[200]\ttrain-mlogloss:0.18648\tval-mlogloss:0.69622\n",
            "[250]\ttrain-mlogloss:0.17249\tval-mlogloss:0.69185\n",
            "[300]\ttrain-mlogloss:0.16464\tval-mlogloss:0.68935\n",
            "[350]\ttrain-mlogloss:0.16056\tval-mlogloss:0.68896\n",
            "[351]\ttrain-mlogloss:0.16037\tval-mlogloss:0.68916\n",
            "for *TUNED* XGBoost\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.66      0.65      0.65        48\n",
            "          tekno       0.67      0.72      0.69        53\n",
            "        warzone       0.75      0.83      0.79        52\n",
            "     industrial       0.82      0.67      0.74        61\n",
            "non-techno-drop       0.92      0.98      0.95        47\n",
            "\n",
            "       accuracy                           0.76       261\n",
            "      macro avg       0.76      0.77      0.76       261\n",
            "   weighted avg       0.76      0.76      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### *TUNED XGBoost for single-label\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd, xgboost as xgb, numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "Dy_train = np.asarray(y_train)\n",
        "Dy_test  = np.asarray(y_test)\n",
        "\n",
        "Dy_train_idx = np.argmax(Dy_train, axis=1)\n",
        "Dy_test_idx  = np.argmax(Dy_test,  axis=1)\n",
        "\n",
        "num_class = Dy_train.shape[1]\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=Dy_train_idx)\n",
        "dval   = xgb.DMatrix(X_test,  label=Dy_test_idx)\n",
        "\n",
        "params = best_params.copy()\n",
        "params.pop(\"n_estimators\", None)\n",
        "params.update(dict(\n",
        "    objective   = 'multi:softprob',\n",
        "    num_class   = len(CLASSES),\n",
        "    eval_metric = 'mlogloss'\n",
        "))\n",
        "\n",
        "\n",
        "booster = xgb.train(params, dtrain,\n",
        "                    num_boost_round=600,\n",
        "                    evals=[(dtrain,'train'), (dval,'val')],\n",
        "                    early_stopping_rounds=30,\n",
        "                    verbose_eval=50)\n",
        "\n",
        "\n",
        "y_pred = booster.predict(dval).argmax(axis=1)\n",
        "\n",
        "#  overall multi class report\n",
        "print(\"for *TUNED* XGBoost\")\n",
        "print(classification_report(Dy_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk5WRZ6VN6xB",
        "outputId": "c9bd048c-84cc-4588-9f62-cddc803b06c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttrain's multi_logloss: 0.0463771\tval's multi_logloss: 0.728026\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttrain's multi_logloss: 0.125694\tval's multi_logloss: 0.710398\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.72      0.65      0.68        48\n",
            "          tekno       0.66      0.70      0.68        53\n",
            "        warzone       0.74      0.83      0.78        52\n",
            "     industrial       0.77      0.67      0.72        61\n",
            "non-techno-drop       0.90      0.98      0.94        47\n",
            "\n",
            "       accuracy                           0.76       261\n",
            "      macro avg       0.76      0.76      0.76       261\n",
            "   weighted avg       0.76      0.76      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new LightGBM for ML\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "y_train_idx = np.argmax(y_train, axis=1)\n",
        "y_test_idx  = np.argmax(y_test, axis=1)\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train_idx)\n",
        "val_set   = lgb.Dataset(X_test,   label=y_test_idx, reference=train_set)\n",
        "\n",
        "params_lgb = dict(\n",
        "    objective        = 'multiclass',\n",
        "    num_class        = len(CLASSES),\n",
        "    metric           = 'multi_logloss',\n",
        "    learning_rate    = 0.05,\n",
        "    num_leaves       = 31,\n",
        "    feature_fraction = 0.9,\n",
        "    bagging_fraction = 0.9,\n",
        "    bagging_freq     = 5,\n",
        "    seed             = SEED,\n",
        "    verbosity        = -1\n",
        ")\n",
        "\n",
        "lgbm = lgb.train(\n",
        "        params_lgb,\n",
        "        train_set,\n",
        "        num_boost_round = 2000,\n",
        "        valid_sets      = [train_set, val_set],\n",
        "        valid_names     = ['train',  'val'],\n",
        "        callbacks       = [lgb.early_stopping(stopping_rounds=100,\n",
        "                                             verbose=True),\n",
        "                           lgb.log_evaluation(period=100)]\n",
        ")\n",
        "\n",
        "# prediction\n",
        "y_pred = np.argmax(lgbm.predict(X_test), axis=1)\n",
        "\n",
        "print(classification_report(y_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-78lHXdYSvF",
        "outputId": "906aaeb2-5df9-46ec-a348-8d725d3581fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best LGBM macro-F1: 0.7680522864037456\n",
            "Parameters: {'reg_lambda': 0, 'reg_alpha': 0, 'n_estimators': 400, 'min_split_gain': 0.2, 'min_child_samples': 10, 'max_depth': 8, 'learning_rate': 0.05, 'feature_fraction': 0.85, 'bagging_freq': 0, 'bagging_fraction': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "### hyper-tunning LightGBM\n",
        "\n",
        "\n",
        "\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth'        : [4, 6, 8, -1],\n",
        "    'learning_rate'    : [0.20, 0.10, 0.05],\n",
        "    'n_estimators'     : [400, 800, 1600],\n",
        "    'feature_fraction' : [0.7, 0.85, 1.0],\n",
        "    'bagging_fraction' : [0.7, 0.85, 1.0],\n",
        "    'bagging_freq'     : [0, 5],\n",
        "    'min_child_samples': [10, 25, 50],\n",
        "    'min_split_gain'   : [0, 0.2],\n",
        "    'reg_alpha'        : [0, 0.5],\n",
        "    'reg_lambda'       : [0, 1.0]\n",
        "}\n",
        "\n",
        "search = ParameterSampler(param_grid, n_iter=25, random_state=SEED)\n",
        "\n",
        "best_f1, best_params = 0, None\n",
        "for p in search:\n",
        "    clf = LGBMClassifier(objective='multiclass',\n",
        "                         num_class=len(CLASSES),\n",
        "                         **p)\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train_idx,\n",
        "        eval_set=[(X_test, y_test_idx)],\n",
        "        eval_metric='multi_logloss',\n",
        "        callbacks=[early_stopping(50, verbose=False),\n",
        "                   log_evaluation(period=0)]\n",
        "    )\n",
        "\n",
        "    f1 = f1_score(y_test_idx, clf.predict(X_test), average='macro')\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_params = f1, p\n",
        "\n",
        "print(\"Best LGBM macro-F1:\", best_f1)\n",
        "print(\"Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhpxaoQacq4g",
        "outputId": "fcd571f9-388c-4930-e301-45d68d805263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttrain's multi_logloss: 0.0704831\tval's multi_logloss: 0.714505\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttrain's multi_logloss: 0.116869\tval's multi_logloss: 0.706085\n",
            "for *TUNED* LightGBM\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.70      0.69      0.69        48\n",
            "          tekno       0.70      0.70      0.70        53\n",
            "        warzone       0.74      0.81      0.77        52\n",
            "     industrial       0.76      0.67      0.71        61\n",
            "non-techno-drop       0.94      1.00      0.97        47\n",
            "\n",
            "       accuracy                           0.77       261\n",
            "      macro avg       0.77      0.77      0.77       261\n",
            "   weighted avg       0.76      0.77      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### *TUNED LightGBM\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train_idx)\n",
        "val_set   = lgb.Dataset(X_test,   label=y_test_idx, reference=train_set)\n",
        "\n",
        "params_lgb = best_params.copy()\n",
        "params_lgb.update({\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(CLASSES),\n",
        "    'metric': 'multi_logloss',\n",
        "    'seed': SEED,\n",
        "    'verbosity': -1\n",
        "})\n",
        "\n",
        "num_boost_round = params_lgb.pop('n_estimators')\n",
        "\n",
        "#  training with callbacks\n",
        "lgbm = lgb.train(\n",
        "    params_lgb,\n",
        "    train_set,\n",
        "    num_boost_round = 1600,\n",
        "    valid_sets      = [train_set, val_set],\n",
        "    valid_names     = ['train', 'val'],\n",
        "    callbacks       = [\n",
        "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
        "        lgb.log_evaluation(period=100)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# prediction\n",
        "y_pred = np.argmax(lgbm.predict(X_test), axis=1)\n",
        "\n",
        "print(\"for *TUNED* LightGBM\")\n",
        "print(classification_report(y_test_idx, y_pred, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zyIVAfEoeoWM",
        "outputId": "3c18bc08-add8-4723-bc4d-49328cf76a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [06:47:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.71      0.67      0.69        48\n",
            "          tekno       0.66      0.70      0.68        53\n",
            "        warzone       0.77      0.83      0.80        52\n",
            "     industrial       0.78      0.69      0.73        61\n",
            "non-techno-drop       0.92      0.98      0.95        47\n",
            "\n",
            "       accuracy                           0.77       261\n",
            "      macro avg       0.77      0.77      0.77       261\n",
            "   weighted avg       0.77      0.77      0.76       261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### soft-vote stacking\n",
        "\n",
        "\n",
        "\n",
        "# xgboost\n",
        "params_xgb = params.copy()\n",
        "params_xgb.update({\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': len(CLASSES),\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'seed': SEED\n",
        "})\n",
        "\n",
        "dtrain_xgb = xgb.DMatrix(X_train, label=y_train_idx)\n",
        "dtest_xgb  = xgb.DMatrix(X_test)\n",
        "\n",
        "booster_xgb = xgb.train(params_xgb, dtrain_xgb, num_boost_round=200)\n",
        "\n",
        "# lgbm\n",
        "params_lgbm = params_lgb.copy()\n",
        "params_lgbm.update({\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(CLASSES),\n",
        "    'metric': 'multi_logloss',\n",
        "    'seed': SEED,\n",
        "    'verbosity': -1\n",
        "})\n",
        "\n",
        "train_set_lgb = lgb.Dataset(X_train, label=y_train_idx)\n",
        "booster_lgb = lgb.train(params_lgbm,\n",
        "                        train_set_lgb,\n",
        "                        num_boost_round=400)\n",
        "\n",
        "# pedict\n",
        "proba_xgb = booster_xgb.predict(dtest_xgb)\n",
        "proba_lgb = booster_lgb.predict(X_test)\n",
        "\n",
        "# soft-vote stacking\n",
        "alpha = 0.5\n",
        "proba_stack = alpha * proba_xgb + (1 - alpha) * proba_lgb\n",
        "y_pred_stack = proba_stack.argmax(axis=1)\n",
        "\n",
        "print(classification_report(y_test_idx, y_pred_stack, target_names=CLASSES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f1DxQfyE2Mwm"
      },
      "outputs": [],
      "source": [
        "# ### creating normal melgrams\n",
        "\n",
        "\n",
        "\n",
        "# import librosa\n",
        "# import librosa.display\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# SR      = 22_050\n",
        "# N_MELS  = 128\n",
        "# F_CUT   = 8_000\n",
        "\n",
        "# def save_melgram(input_path, output_path):\n",
        "\n",
        "#     # loading the audio file\n",
        "#     y, sr = librosa.load(input_path, sr=22_050)\n",
        "\n",
        "#     # generating the mel spectograms\n",
        "#     S = librosa.feature.melspectrogram(y=y, sr=sr,\n",
        "#                                        n_fft=1024,\n",
        "#                                        hop_length=512,\n",
        "#                                        n_mels=128,\n",
        "#                                        fmin=20,\n",
        "#                                        fmax=11_000)\n",
        "#     S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "#     # plotting the spectograms\n",
        "#     plt.figure(figsize=(3, 3))\n",
        "#     librosa.display.specshow(S_db, sr=sr, hop_length=512,\n",
        "#                              x_axis='time', y_axis='mel',\n",
        "#                              fmax=11_000, cmap='gray_r')   # grayscale\n",
        "\n",
        "#     plt.axis('off')  # removing the axis\n",
        "#     plt.tight_layout(pad=0)\n",
        "\n",
        "#     # saving\n",
        "#     plt.savefig(output_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close()\n",
        "\n",
        "# # going through the clips, to make the spectograms\n",
        "# for filename in os.listdir(clips_dir):\n",
        "#     if filename.endswith(\".mp3\"):\n",
        "#         clip_path = os.path.join(clips_dir, filename)\n",
        "#         melgram_path = os.path.join(melgrams_dir, filename[:-4] + \".png\")\n",
        "#         save_melgram(clip_path, melgram_path)\n",
        "#         print(f\"Melgram saved for {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2BaQtieXq9HI"
      },
      "outputs": [],
      "source": [
        "# ### creating melgrams with low-pass filter\n",
        "\n",
        "\n",
        "\n",
        "# import librosa\n",
        "# import librosa.display\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# SR      = 22_050\n",
        "# N_MELS  = 128\n",
        "# F_CUT   = 2_000        # low-pass corner\n",
        "\n",
        "# def save_melgram(input_path, output_path):\n",
        "\n",
        "#     # loading the audio file\n",
        "#     y, _ = librosa.load(input_path, sr=SR)\n",
        "\n",
        "#     # generating the mel spectograms\n",
        "\n",
        "#     M = librosa.feature.melspectrogram(\n",
        "#             y=y, sr=SR,\n",
        "#             n_fft=1024,\n",
        "#             hop_length=512,\n",
        "#             n_mels=N_MELS,\n",
        "#             fmin=20,\n",
        "#             fmax=F_CUT)\n",
        "\n",
        "#     M_db = librosa.power_to_db(M, ref=np.max).astype(np.float32)\n",
        "\n",
        "#     # plot PNG > grayscale\n",
        "#     plt.figure(figsize=(3, 3))\n",
        "#     librosa.display.specshow(M_db, sr=SR,\n",
        "#                              hop_length=512,\n",
        "#                              x_axis='time', y_axis='mel',\n",
        "#                              fmax=F_CUT,\n",
        "#                              cmap='gray_r')\n",
        "#     plt.axis('off')\n",
        "#     plt.tight_layout(pad=0)\n",
        "#     plt.savefig(output_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close()\n",
        "\n",
        "# # going through the clips, to make the spectograms\n",
        "# for filename in os.listdir(clips_dir):\n",
        "#     if filename.endswith(\".mp3\"):\n",
        "#         clip_path = os.path.join(clips_dir, filename)\n",
        "#         low_melgram_path = os.path.join(low_melgrams_dir, filename[:-4] + \".png\")\n",
        "#         save_melgram(clip_path, low_melgram_path)\n",
        "#         print(f\"Melgram saved for {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzSw_DSU759L",
        "outputId": "762fb799-1702-42fd-acef-129013e6a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/melgrams\n",
            "1772\n"
          ]
        }
      ],
      "source": [
        "### see how many we melgrams we got after all\n",
        "#1772 ?\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "n_written = sum(1 for f in os.listdir(low_melgrams_dir))\n",
        "print(melgrams_dir)\n",
        "print(n_written)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGOcyMLcV4e"
      },
      "outputs": [],
      "source": [
        "# ### connect to gdrive\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !ls \"/content/drive/MyDrive/MSc/deep_learning/apostolis/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVKqp3f41lmk",
        "outputId": "c5ada6ec-a54f-4a19-a52b-74adeae53917"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'norm_melgrams_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m low_melgrams_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/low_melgrams\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# melgrams_dir = low_melgrams_dir # low_melgrams_dir , norm_melgrams_dir\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m all_mels \u001b[38;5;241m=\u001b[39m [\u001b[43mnorm_melgrams_dir\u001b[49m, low_melgrams_dir]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(clips_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'norm_melgrams_dir' is not defined"
          ]
        }
      ],
      "source": [
        "### setting input files and folder-dirs\n",
        "\n",
        "\n",
        "\n",
        "CLASSES = ['bouncy','tekno','warzone','industrial','non-techno-drop']\n",
        "# REMOVED: 'psytech',\n",
        "\n",
        "# csv_file_path = \"/content/drive/MyDrive/MSc/deep_learning/labels_final.csv\"\n",
        "# tracks_dir = \"/content/drive/MyDrive/MSc/deep_learning/tracks_final\"\n",
        "# clips_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/clips\"\n",
        "\n",
        "# norm_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/norm_melgrams_dir\"\n",
        "# low_melgrams_dir = \"/content/drive/MyDrive/MSc/deep_learning/apostolis/low_melgrams\"\n",
        "# melgrams_dir = norm_melgrams_dir # low_melgrams_dir\n",
        "\n",
        "csv_file_path = \"data/labels_final.csv\"\n",
        "tracks_dir = \"data/tracks_final\"\n",
        "clips_dir = \"data/clips\"\n",
        "\n",
        "melgrams_dir = \"data/melgrams\"\n",
        "low_melgrams_dir = \"data/low_melgrams\"\n",
        "# melgrams_dir = low_melgrams_dir # low_melgrams_dir , norm_melgrams_dir\n",
        "all_mels = [norm_melgrams_dir, low_melgrams_dir]\n",
        "\n",
        "\n",
        "import os\n",
        "os.makedirs(clips_dir, exist_ok=True)\n",
        "os.makedirs(norm_melgrams_dir, exist_ok=True)\n",
        "os.makedirs(low_melgrams_dir, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# import random, numpy as np\n",
        "# from tensorflow.keras.utils import set_random_seed\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# tf.random.set_seed(SEED)\n",
        "# set_random_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyyDl2ApO-Ll"
      },
      "outputs": [],
      "source": [
        "### mapping labels to melgrams\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# loading the csv with the labels\n",
        "df_csv = pd.read_csv(csv_file_path)\n",
        "\n",
        "# creating a dictionary to keep the mapping\n",
        "melgram_label_mapping = {}\n",
        "\n",
        "# going through the csv\n",
        "for index, row in df_csv.iterrows():\n",
        "    sample_id = str(row['sample_id'])\n",
        "    melgram_filename = sample_id + \".png\"\n",
        "\n",
        "    labels = row[CLASSES].values.tolist()\n",
        "\n",
        "    # check to ensure at least one label is 1\n",
        "    if any(label == 1 for label in labels):\n",
        "        melgram_label_mapping[melgram_filename] = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTaT2oGjxmyx",
        "outputId": "48ba80c0-dd9d-4205-ca10-7da64ddcb417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tensors: (1305, 300, 300, 1)\n",
            "\n",
            "Train clips : 872\n",
            "Valid clips : 207\n",
            "Test  clips : 226\n",
            "\n",
            "Train labels : 872\n",
            "Valid labels : 207\n",
            "Test  labels : 226\n"
          ]
        }
      ],
      "source": [
        "# prep-split for train/val/test\n",
        "\n",
        "\n",
        "\n",
        "# build X, y + a 'groups' list with song IDs, to ensure no same-song clips fall in different train/val/test sets !\n",
        "\n",
        "from pathlib import Path\n",
        "import cv2, numpy as np, pandas as pd, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "image_size = (300, 300)\n",
        "input_shape = (300, 300, 1)\n",
        "labels_list = CLASSES\n",
        "\n",
        "df_csv = pd.read_csv(csv_file_path).set_index('sample_id')\n",
        "\n",
        "def list_to_idx(vec):\n",
        "    idxs = [i for i, v in enumerate(vec) if v == 1]\n",
        "    return idxs[0] if len(idxs) == 1 else None\n",
        "\n",
        "X, y_int, groups = [], [], []\n",
        "\n",
        "for fname, vec in melgram_label_mapping.items():\n",
        "    class_idx = list_to_idx(vec)\n",
        "    if class_idx is None:\n",
        "        continue\n",
        "\n",
        "    png_path = os.path.join(melgrams_dir, fname)\n",
        "    if not os.path.exists(png_path):\n",
        "        continue\n",
        "\n",
        "    # load & preprocess to (128,128,1)\n",
        "    img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, image_size, interpolation=cv2.INTER_AREA)\n",
        "    img = np.expand_dims(img, -1).astype(np.float32) / 255.0\n",
        "\n",
        "    X.append(img)\n",
        "    y_int.append(class_idx)\n",
        "\n",
        "    sample_id = Path(fname).stem\n",
        "    groups.append(df_csv.loc[sample_id, 'spot_id'])\n",
        "\n",
        "X       = np.array(X,       dtype=np.float32)\n",
        "y_int   = np.array(y_int,  dtype=np.int32)\n",
        "groups  = np.array(groups)\n",
        "\n",
        "print(\"Loaded tensors:\", X.shape)\n",
        "\n",
        "# first split, 70% train, 30% temp (val+test)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "train_idx, temp_idx = next(sgkf.split(X, y_int, groups=groups))\n",
        "\n",
        "# 2nd split, 50/50 on temp → 15% val, 15% test\n",
        "\n",
        "sgkf_temp = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
        "val_idx, test_idx = next(sgkf_temp.split(X[temp_idx], y_int[temp_idx], groups=groups[temp_idx]))\n",
        "\n",
        "X_train, y_train = X[train_idx], y_int[train_idx]\n",
        "X_val, y_val = X[temp_idx][val_idx], y_int[temp_idx][val_idx]\n",
        "X_test, y_test = X[temp_idx][test_idx], y_int[temp_idx][test_idx]\n",
        "\n",
        "print(f\"\\nTrain clips : {len(X_train)}\")\n",
        "print(f\"Valid clips : {len(X_val)}\")\n",
        "print(f\"Test  clips : {len(X_test)}\")\n",
        "\n",
        "print(f\"\\nTrain labels : {len(y_train)}\")\n",
        "print(f\"Valid labels : {len(y_val)}\")\n",
        "print(f\"Test  labels : {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JQ9R7FbwHutH"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9mVIOskGu3e",
        "outputId": "7ee97b8c-3538-4223-bb13-0b16690173cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in y_train: [0 1 2 3 4]\n",
            "X_train shape: (872, 300, 300, 1)\n",
            "y_train shape: (872,)\n",
            "Sample y_train: 3\n",
            "y_train dtype: int32\n"
          ]
        }
      ],
      "source": [
        "### train-set check + build metric-printers\n",
        "\n",
        "\n",
        "\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"Sample y_train:\", y_train[0])\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# evaluating the model on the validation & the test set\n",
        "\n",
        "def show_val_results(history, X_val, y_val, class_names):\n",
        "\n",
        "    # epoch with the lowest val-loss\n",
        "\n",
        "    best = np.argmin(history.history['val_loss'])\n",
        "    print(f\"\\nBest epoch = {best+1}\")\n",
        "    print(f\"  train_acc {history.history['accuracy'][best]:.4f}\"\n",
        "          f\" | train_loss {history.history['loss'][best]:.4f}\")\n",
        "    print(f\"  val_acc   {history.history['val_accuracy'][best]:.4f}\"\n",
        "          f\" | val_loss  {history.history['val_loss'][best]:.4f}\")\n",
        "\n",
        "    y_pred = history.model.predict(X_val, verbose=0).argmax(axis=1)\n",
        "    print(f\"\\nWeighted F1 (val) : {f1_score(y_val, y_pred, average='weighted'):.4f}\")\n",
        "    print(classification_report(y_val, y_pred, target_names=class_names))\n",
        "    print(\"Confusion matrix (val):\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "\n",
        "def show_test_results(modelo, X_test, y_test, class_names):\n",
        "\n",
        "    y_pred = modelo.predict(X_test, verbose=0).argmax(axis=1)\n",
        "\n",
        "    print(\"\\n=== TEST-SET RESULTS ===\")\n",
        "    print(f\"Weighted F1 (test): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "    print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-63NhDCOvUtI",
        "outputId": "c6b699be-c0f2-4ef3-8371-8e1aee8a4f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train class counts: {'bouncy': np.int64(184), 'tekno': np.int64(183), 'warzone': np.int64(177), 'industrial': np.int64(170), 'non-techno-drop': np.int64(158)}\n",
            "Val class counts: {'bouncy': np.int64(35), 'tekno': np.int64(30), 'warzone': np.int64(45), 'industrial': np.int64(47), 'non-techno-drop': np.int64(50)}\n",
            "Test class counts: {'bouncy': np.int64(42), 'tekno': np.int64(46), 'warzone': np.int64(44), 'industrial': np.int64(42), 'non-techno-drop': np.int64(52)}\n"
          ]
        }
      ],
      "source": [
        "### check & verify class distributions\n",
        "\n",
        "\n",
        "def print_class_dist(y, name):\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    print(f\"{name} class counts:\", dict(zip(labels_list, counts)))\n",
        "\n",
        "print_class_dist(y_train, \"Train\")\n",
        "print_class_dist(y_val, \"Val\")\n",
        "print_class_dist(y_test, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2amEnqMvUtI"
      },
      "outputs": [],
      "source": [
        "### class weight to handle class imbalance, especially in the val & test sets\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b-3W_5yqHnGM",
        "outputId": "c9b20aec-6076-403c-eea7-3443c582b599"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "14/14 - 42s - 3s/step - accuracy: 0.2144 - loss: 1.9325 - val_accuracy: 0.2077 - val_loss: 1.5937\n",
            "Epoch 2/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.3268 - loss: 1.5086 - val_accuracy: 0.2367 - val_loss: 1.5286\n",
            "Epoch 3/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.4438 - loss: 1.3192 - val_accuracy: 0.6232 - val_loss: 1.0644\n",
            "Epoch 4/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.5860 - loss: 1.0120 - val_accuracy: 0.8261 - val_loss: 0.8537\n",
            "Epoch 5/50\n",
            "14/14 - 34s - 2s/step - accuracy: 0.7087 - loss: 0.8249 - val_accuracy: 0.7053 - val_loss: 0.8046\n",
            "Epoch 6/50\n",
            "14/14 - 37s - 3s/step - accuracy: 0.7339 - loss: 0.7340 - val_accuracy: 0.7681 - val_loss: 0.6662\n",
            "Epoch 7/50\n",
            "14/14 - 34s - 2s/step - accuracy: 0.7833 - loss: 0.6399 - val_accuracy: 0.7778 - val_loss: 0.6345\n",
            "Epoch 8/50\n",
            "14/14 - 34s - 2s/step - accuracy: 0.8028 - loss: 0.5808 - val_accuracy: 0.7778 - val_loss: 0.6312\n",
            "Epoch 9/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.8463 - loss: 0.4445 - val_accuracy: 0.8406 - val_loss: 0.5201\n",
            "Epoch 10/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.8865 - loss: 0.3654 - val_accuracy: 0.8309 - val_loss: 0.5076\n",
            "Epoch 11/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9117 - loss: 0.3014 - val_accuracy: 0.8647 - val_loss: 0.4258\n",
            "Epoch 12/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.9323 - loss: 0.2347 - val_accuracy: 0.8357 - val_loss: 0.4633\n",
            "Epoch 13/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.9438 - loss: 0.1816 - val_accuracy: 0.8502 - val_loss: 0.3982\n",
            "Epoch 14/50\n",
            "14/14 - 36s - 3s/step - accuracy: 0.9415 - loss: 0.1653 - val_accuracy: 0.8792 - val_loss: 0.3780\n",
            "Epoch 15/50\n",
            "14/14 - 34s - 2s/step - accuracy: 0.9599 - loss: 0.1402 - val_accuracy: 0.8406 - val_loss: 0.4590\n",
            "Epoch 16/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9553 - loss: 0.1345 - val_accuracy: 0.8261 - val_loss: 0.5048\n",
            "Epoch 17/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9576 - loss: 0.1336 - val_accuracy: 0.8164 - val_loss: 0.5702\n",
            "Epoch 18/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9748 - loss: 0.0889 - val_accuracy: 0.8841 - val_loss: 0.4207\n",
            "Epoch 19/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.9874 - loss: 0.0408 - val_accuracy: 0.8502 - val_loss: 0.5223\n",
            "Epoch 20/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9897 - loss: 0.0421 - val_accuracy: 0.8406 - val_loss: 0.5430\n",
            "Epoch 21/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.9897 - loss: 0.0405 - val_accuracy: 0.8261 - val_loss: 0.5719\n",
            "Epoch 22/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9920 - loss: 0.0344 - val_accuracy: 0.8261 - val_loss: 0.5396\n",
            "Epoch 23/50\n",
            "14/14 - 32s - 2s/step - accuracy: 0.9874 - loss: 0.0398 - val_accuracy: 0.8599 - val_loss: 0.4276\n",
            "Epoch 24/50\n",
            "14/14 - 33s - 2s/step - accuracy: 0.9966 - loss: 0.0247 - val_accuracy: 0.8454 - val_loss: 0.5042\n",
            "\n",
            "Best epoch = 14\n",
            "  train_acc 0.9415 | train_loss 0.1653\n",
            "  val_acc   0.8792 | val_loss  0.3780\n",
            "\n",
            "Weighted F1 (val) : 0.8781\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.86      0.89      0.87        35\n",
            "          tekno       0.92      0.77      0.84        30\n",
            "        warzone       0.84      0.96      0.90        45\n",
            "     industrial       0.82      0.79      0.80        47\n",
            "non-techno-drop       0.96      0.96      0.96        50\n",
            "\n",
            "       accuracy                           0.88       207\n",
            "      macro avg       0.88      0.87      0.87       207\n",
            "   weighted avg       0.88      0.88      0.88       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[31  0  0  4  0]\n",
            " [ 3 23  0  4  0]\n",
            " [ 0  1 43  0  1]\n",
            " [ 2  1  6 37  1]\n",
            " [ 0  0  2  0 48]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.8202\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.92      0.79      0.85        42\n",
            "          tekno       0.71      0.78      0.74        46\n",
            "        warzone       0.80      0.89      0.84        44\n",
            "     industrial       0.71      0.69      0.70        42\n",
            "non-techno-drop       0.98      0.92      0.95        52\n",
            "\n",
            "       accuracy                           0.82       226\n",
            "      macro avg       0.82      0.81      0.82       226\n",
            "   weighted avg       0.83      0.82      0.82       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[33  1  0  8  0]\n",
            " [ 0 36  7  3  0]\n",
            " [ 0  4 39  1  0]\n",
            " [ 0  9  3 29  1]\n",
            " [ 3  1  0  0 48]]\n"
          ]
        }
      ],
      "source": [
        "### main CNN construction\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf, random\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "random.seed(SEED);\n",
        "np.random.seed(SEED)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "    MaxPooling2D(2,2), # MAYBE (2,1) OR (1,2) ?\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels_list), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history1 = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "show_val_results(history1, X_val, y_val, CLASSES)\n",
        "show_test_results(model,   X_test, y_test, CLASSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNe5QSQHhFT9"
      },
      "outputs": [],
      "source": [
        "### imports + constants for pipeline\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import os, random, cv2, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "CSV_FILE_PATH = \"data/labels_final.csv\"\n",
        "CLASSES = ['bouncy','tekno','warzone','industrial','non-techno-drop']\n",
        "\n",
        "mel_dirs = [\"data/melgrams\", \"data/low_melgrams\"]\n",
        "image_sizes = [(128, 128), (256, 256)]\n",
        "\n",
        "SEED      = 42\n",
        "N_RUNS    = 5\n",
        "EPOCHS    = 50\n",
        "BATCH     = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDdvyih6hFT-"
      },
      "outputs": [],
      "source": [
        "### data prep + split functions\n",
        "\n",
        "\n",
        "\n",
        "# mapping labels to melgrams\n",
        "\n",
        "df_csv = pd.read_csv(CSV_FILE_PATH)\n",
        "melgram_label_mapping = {}\n",
        "\n",
        "# going through the csv\n",
        "for index, row in df_csv.iterrows():\n",
        "    sample_id = str(row['sample_id'])\n",
        "    melgram_filename = sample_id + \".png\"\n",
        "\n",
        "    labels = row[CLASSES].values.tolist()\n",
        "\n",
        "    # check to ensure at least one label is 1\n",
        "    if any(label == 1 for label in labels):\n",
        "        melgram_label_mapping[melgram_filename] = labels\n",
        "\n",
        "\n",
        "def one_hot_to_index(v):\n",
        "    v_arr = np.asarray(v)\n",
        "    if v_arr.ndim == 0:\n",
        "        return int(v_arr)\n",
        "    hits = np.where(v_arr == 1)[0]\n",
        "    return int(hits[0]) if len(hits) == 1 else None\n",
        "\n",
        "\n",
        "# load PNGs + stratified group split\n",
        "\n",
        "def prepare_data(mel_dir, image_size, seed):\n",
        "    df_csv = pd.read_csv(CSV_FILE_PATH).set_index('sample_id')\n",
        "\n",
        "    X, y_int, groups = [], [], []\n",
        "\n",
        "    for fname, vec in melgram_label_mapping.items():\n",
        "        class_idx = one_hot_to_index(vec)\n",
        "        png_path = os.path.join(mel_dir, fname)\n",
        "        if not os.path.exists(png_path):\n",
        "            continue\n",
        "\n",
        "        # load & resize to the requested resolution\n",
        "        img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, image_size, interpolation=cv2.INTER_AREA)\n",
        "        img = np.expand_dims(img, -1).astype(np.float32) / 255.0\n",
        "\n",
        "        X.append(img)\n",
        "        y_int.append(class_idx)\n",
        "\n",
        "        sample_id = Path(fname).stem\n",
        "        groups.append(df_csv.loc[sample_id, 'spot_id'])\n",
        "\n",
        "    X, y_int, groups = map(np.array, (X, y_int, groups))\n",
        "\n",
        "    # 70 / 30 split - stratified & group-aware\n",
        "    sgkf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=seed)\n",
        "    train_idx, temp_idx = next(sgkf.split(X, y_int, groups))\n",
        "\n",
        "    # 15 / 15 split on the 30 % temp\n",
        "    sgkf_temp = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=seed)\n",
        "    val_idx, test_idx = next(sgkf_temp.split(X[temp_idx], y_int[temp_idx], groups[temp_idx]))\n",
        "\n",
        "    X_train, y_train = X[train_idx], y_int[train_idx]\n",
        "    X_val, y_val = X[temp_idx][val_idx], y_int[temp_idx][val_idx]\n",
        "    X_test, y_test = X[temp_idx][test_idx], y_int[temp_idx][test_idx]\n",
        "\n",
        "    print(f\"[{Path(mel_dir).name} | {image_size[0]}x{image_size[1]}] \"\n",
        "          f\"train={len(X_train)}  val={len(X_val)}  test={len(X_test)}\")\n",
        "\n",
        "    ### class weight to handle class imbalance, especially in the val & test sets\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    return (X_train, y_train, X_val, y_val, X_test, y_test, class_weights)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX26pL7zhFT-"
      },
      "outputs": [],
      "source": [
        "### run-CNNs / helper functions for the 20-run for each CNN\n",
        "\n",
        "\n",
        "\n",
        "# 1-single run > new model + new RNGs\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "def run_once(run_idx, X_train, y_train, X_val, y_val, X_test, y_test, class_weights, build, input_shape):\n",
        "\n",
        "    K.clear_session()\n",
        "    fresh_seed = SEED + run_idx\n",
        "    random.seed(fresh_seed)\n",
        "    np.random.seed(fresh_seed)\n",
        "    # tf.random.set_seed(SEED) # NOT THIS ONE, for some randomness\n",
        "\n",
        "    model = build(input_shape)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "              validation_data=(X_val, y_val),\n",
        "              epochs=EPOCHS,\n",
        "              batch_size=BATCH,\n",
        "              shuffle=True,\n",
        "              callbacks=[early_stop],\n",
        "              class_weight=class_weights,\n",
        "              verbose=0)\n",
        "\n",
        "    # evaluating on the unseen test set\n",
        "    y_pred = model.predict(X_test, verbose=0).argmax(axis=1)\n",
        "    w_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(f\"    run {run_idx+1}: F1 = {w_f1:.4f}\")\n",
        "    return w_f1\n",
        "\n",
        "# MAIN 3-LOOP ATTEMPT\n",
        "\n",
        "def full_test(build):\n",
        "    # loop for mel type\n",
        "    for mel_dir in mel_dirs:\n",
        "\n",
        "        # loop for dimensions\n",
        "        for image_size in image_sizes:\n",
        "            f1_scores = []\n",
        "\n",
        "            # loop w/ new seed + 5 runs\n",
        "            for r in range(N_RUNS):\n",
        "\n",
        "                # load & split once per mel + size combo\n",
        "\n",
        "                (X_train, y_train, X_val, y_val, X_test, y_test, class_weights) = prepare_data(mel_dir, image_size, seed = SEED + r)\n",
        "                f1 = run_once(r, X_train, y_train, X_val, y_val, X_test, y_test, class_weights, build=build, input_shape=(*image_size, 1))\n",
        "                f1_scores.append(f1)\n",
        "\n",
        "            print(f\"\\n=== SUMMARY [{Path(mel_dir).name} | \"\n",
        "                f\"{image_size[0]}x{image_size[1]}] ===\")\n",
        "            print(\"all F1s:\", [f\"{x:.4f}\" for x in f1_scores])\n",
        "            print(f\"mean +/- std: {np.mean(f1_scores):.4f} +/- \"\n",
        "                f\"{np.std(f1_scores):.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs2aN2kDhFT-",
        "outputId": "ca13787e-ae23-4d3c-e377-28a7ee2d412d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[melgrams | 128x128] train=872  val=207  test=226\n",
            "WARNING:tensorflow:From c:\\Users\\nonee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "    run 1: F1 = 0.8084\n",
            "[melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7680\n",
            "[melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7652\n",
            "[melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7154\n",
            "[melgrams | 128x128] train=841  val=244  test=220\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C5CB33E830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    run 5: F1 = 0.7886\n",
            "\n",
            "=== SUMMARY [melgrams | 128x128] ===\n",
            "all F1s: ['0.8084', '0.7680', '0.7652', '0.7154', '0.7886']\n",
            "mean +/- std: 0.7692 +/- 0.0311\n",
            "\n",
            "[melgrams | 256x256] train=872  val=207  test=226\n",
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C5CB999090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    run 1: F1 = 0.7427\n",
            "[melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.8115\n",
            "[melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.8277\n",
            "[melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6881\n",
            "[melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7459\n",
            "\n",
            "=== SUMMARY [melgrams | 256x256] ===\n",
            "all F1s: ['0.7427', '0.8115', '0.8277', '0.6881', '0.7459']\n",
            "mean +/- std: 0.7632 +/- 0.0507\n",
            "\n",
            "[low_melgrams | 128x128] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7379\n",
            "[low_melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7683\n",
            "[low_melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7228\n",
            "[low_melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7558\n",
            "[low_melgrams | 128x128] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7932\n",
            "\n",
            "=== SUMMARY [low_melgrams | 128x128] ===\n",
            "all F1s: ['0.7379', '0.7683', '0.7228', '0.7558', '0.7932']\n",
            "mean +/- std: 0.7556 +/- 0.0244\n",
            "\n",
            "[low_melgrams | 256x256] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7726\n",
            "[low_melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7796\n",
            "[low_melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7869\n",
            "[low_melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6714\n",
            "[low_melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7426\n",
            "\n",
            "=== SUMMARY [low_melgrams | 256x256] ===\n",
            "all F1s: ['0.7726', '0.7796', '0.7869', '0.6714', '0.7426']\n",
            "mean +/- std: 0.7506 +/- 0.0424\n",
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "### main/initial architecture\n",
        "\n",
        "\n",
        "\n",
        "# FULL 20-RUN PIPELINE\n",
        "\n",
        "def cnn_main(input_shape):\n",
        "    return Sequential([\n",
        "        Input(input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "full_test(cnn_main)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEj-RvBe4KJl",
        "outputId": "78bc5e35-bcf8-4803-83a5-78918304583f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[melgrams | 128x128] train=872  val=207  test=226\n",
            "WARNING:tensorflow:From c:\\Users\\nonee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "    run 1: F1 = 0.7688\n",
            "[melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.8140\n",
            "[melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7768\n",
            "[melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7237\n",
            "[melgrams | 128x128] train=841  val=244  test=220\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000255336528C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    run 5: F1 = 0.8008\n",
            "\n",
            "=== SUMMARY [melgrams | 128x128] ===\n",
            "all F1s: ['0.7688', '0.8140', '0.7768', '0.7237', '0.8008']\n",
            "mean +/- std: 0.7768 +/- 0.0311\n",
            "\n",
            "[melgrams | 256x256] train=872  val=207  test=226\n",
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025533CB1120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    run 1: F1 = 0.7576\n",
            "[melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7711\n",
            "[melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.8116\n",
            "[melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.5639\n",
            "[melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.8129\n",
            "\n",
            "=== SUMMARY [melgrams | 256x256] ===\n",
            "all F1s: ['0.7576', '0.7711', '0.8116', '0.5639', '0.8129']\n",
            "mean +/- std: 0.7434 +/- 0.0924\n",
            "\n",
            "[low_melgrams | 128x128] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7711\n",
            "[low_melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.6985\n",
            "[low_melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7081\n",
            "[low_melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6500\n",
            "[low_melgrams | 128x128] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7336\n",
            "\n",
            "=== SUMMARY [low_melgrams | 128x128] ===\n",
            "all F1s: ['0.7711', '0.6985', '0.7081', '0.6500', '0.7336']\n",
            "mean +/- std: 0.7123 +/- 0.0400\n",
            "\n",
            "[low_melgrams | 256x256] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7358\n",
            "[low_melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7367\n",
            "[low_melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7489\n",
            "[low_melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7512\n",
            "[low_melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.6282\n",
            "\n",
            "=== SUMMARY [low_melgrams | 256x256] ===\n",
            "all F1s: ['0.7358', '0.7367', '0.7489', '0.7512', '0.6282']\n",
            "mean +/- std: 0.7202 +/- 0.0464\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### new CNN Dropout>Flatten (2,2) ###\n",
        "\n",
        "\n",
        "\n",
        "# FULL 20-RUN PIPELINE\n",
        "\n",
        "def cnn_drop_first(input_shape):\n",
        "    return Sequential([\n",
        "        Input(input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "full_test(cnn_drop_first)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf25EhMDhFT_",
        "outputId": "cb548ab8-3b4a-4238-f616-77cd5c93dbc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[melgrams | 128×128] train=872  val=207  test=226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    run 1: F1 = 0.0635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    run 2: F1 = 0.0582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    run 3: F1 = 0.1037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    run 4: F1 = 0.0807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    run 5: F1 = 0.0846\n",
            "\n",
            "=== SUMMARY [melgrams | 128×128] ===\n",
            "all F1s: ['0.0635', '0.0582', '0.1037', '0.0807', '0.0846']\n",
            "mean ± std: 0.0781 ± 0.0162\n",
            "\n",
            "[melgrams | 256×256] train=872  val=207  test=226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\annan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# ### 1111 rebuilt CNN Dropout>Flatten (2,2) ###\n",
        "\n",
        "\n",
        "############ something has gone terribly wrong(!), *ignoring* this model ###############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # FULL 20-RUN PIPELINE\n",
        "\n",
        "# # MODEL builder that adapts to `input_shape`\n",
        "\n",
        "# def cnn_narrow(input_shape):\n",
        "#     return Sequential([\n",
        "#         Input(input_shape),\n",
        "#         Conv2D(256, (5,5), activation='relu', padding='same'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(2,2),\n",
        "\n",
        "#         Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(2,2),\n",
        "\n",
        "#         Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(2,2),\n",
        "\n",
        "#         Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "#         BatchNormalization(),\n",
        "\n",
        "#         GlobalAveragePooling2D(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(len(CLASSES), activation='softmax')\n",
        "#     ])\n",
        "\n",
        "# # MAIN 3-LOOP ATTEMPT\n",
        "\n",
        "# # loop for mel type\n",
        "# for mel_dir in mel_dirs:\n",
        "\n",
        "#     # loop for dimensions\n",
        "#     for image_size in image_sizes:\n",
        "#         f1_scores = []\n",
        "\n",
        "#         # loop w/ new seed + 5 runs\n",
        "#         for r in range(N_RUNS):\n",
        "\n",
        "#             # load & split once per mel + size combo\n",
        "\n",
        "#             (X_train, y_train, X_val, y_val, X_test, y_test, class_weights) = prepare_data(mel_dir, image_size, seed = SEED + r)\n",
        "#             f1 = run_once(r, X_train, y_train, X_val, y_val, X_test, y_test, class_weights, build=cnn_narrow, input_shape=(*image_size, 1))\n",
        "#             f1_scores.append(f1)\n",
        "\n",
        "#         print(f\"\\n=== SUMMARY [{Path(mel_dir).name} | \"\n",
        "#               f\"{image_size[0]}x{image_size[1]}] ===\")\n",
        "#         print(\"all F1s:\", [f\"{x:.4f}\" for x in f1_scores])\n",
        "#         print(f\"mean +/- std: {np.mean(f1_scores):.4f} +/- \"\n",
        "#               f\"{np.std(f1_scores):.4f}\\n\")\n",
        "\n",
        "\n",
        "# # #################\n",
        "\n",
        "# # # results with new-CNN for normal melgrams after 5 runs, WITHOUT fixed tf.seed\n",
        "# SUMMARY [melgrams | 128x128]\n",
        "# all F1s: ['0.0635', '0.0582', '0.1037', '0.0807', '0.0846']\n",
        "# mean +/- std: 0.0781 +/- 0.0162\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############ something has gone terribly wrong(!), *ignoring* this model ###############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "LQnhb99i73wz",
        "outputId": "fa7965b1-ffd8-4809-e3d1-1f83339a829c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[melgrams | 128x128] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.8212\n",
            "[melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.8025\n",
            "[melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7107\n",
            "[melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6536\n",
            "[melgrams | 128x128] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.8040\n",
            "\n",
            "=== SUMMARY [melgrams | 128x128] ===\n",
            "all F1s: ['0.8212', '0.8025', '0.7107', '0.6536', '0.8040']\n",
            "mean +/- std: 0.7584 +/- 0.0652\n",
            "\n",
            "[melgrams | 256x256] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7098\n",
            "[melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7429\n",
            "[melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.8223\n",
            "[melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6135\n",
            "[melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7860\n",
            "\n",
            "=== SUMMARY [melgrams | 256x256] ===\n",
            "all F1s: ['0.7098', '0.7429', '0.8223', '0.6135', '0.7860']\n",
            "mean +/- std: 0.7349 +/- 0.0717\n",
            "\n",
            "[low_melgrams | 128x128] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7990\n",
            "[low_melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7839\n",
            "[low_melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7726\n",
            "[low_melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7354\n",
            "[low_melgrams | 128x128] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.7413\n",
            "\n",
            "=== SUMMARY [low_melgrams | 128x128] ===\n",
            "all F1s: ['0.7990', '0.7839', '0.7726', '0.7354', '0.7413']\n",
            "mean +/- std: 0.7664 +/- 0.0245\n",
            "\n",
            "[low_melgrams | 256x256] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.7183\n",
            "[low_melgrams | 256x256] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.7373\n",
            "[low_melgrams | 256x256] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.7962\n",
            "[low_melgrams | 256x256] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.7102\n",
            "[low_melgrams | 256x256] train=841  val=244  test=220\n",
            "    run 5: F1 = 0.6940\n",
            "\n",
            "=== SUMMARY [low_melgrams | 256x256] ===\n",
            "all F1s: ['0.7183', '0.7373', '0.7962', '0.7102', '0.6940']\n",
            "mean +/- std: 0.7312 +/- 0.0354\n",
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "### previous-ultimate model?! not performing so well anymore\n",
        "### >> focus on frequencies-bands rather than time/rhythm\n",
        "\n",
        "\n",
        "\n",
        "# FULL 20-RUN PIPELINE\n",
        "\n",
        "def cnn_1_2(input_shape):\n",
        "    return Sequential([\n",
        "        Input(input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(1, 2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "full_test(cnn_1_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzJGbfRohFT_",
        "outputId": "5183b97b-0564-4fb7-d1b6-3ca52b8f2b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[melgrams | 128x128] train=872  val=207  test=226\n",
            "    run 1: F1 = 0.6957\n",
            "[melgrams | 128x128] train=865  val=216  test=224\n",
            "    run 2: F1 = 0.6642\n",
            "[melgrams | 128x128] train=884  val=191  test=230\n",
            "    run 3: F1 = 0.6419\n",
            "[melgrams | 128x128] train=861  val=239  test=205\n",
            "    run 4: F1 = 0.6904\n",
            "[melgrams | 128x128] train=841  val=244  test=220\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D54E8401F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    run 5: F1 = 0.7201\n",
            "\n",
            "=== SUMMARY [melgrams | 128x128] ===\n",
            "all F1s: ['0.6957', '0.6642', '0.6419', '0.6904', '0.7201']\n",
            "mean +/- std: 0.6825 +/- 0.0270\n",
            "\n",
            "[melgrams | 256x256] train=872  val=207  test=226\n"
          ]
        }
      ],
      "source": [
        "### diff previous-ultimate model?! not performing so well anymore\n",
        "### >> focus on time/rhythm rather than frequencies-bands\n",
        "\n",
        "\n",
        "\n",
        "# FULL 20-RUN PIPELINE\n",
        "\n",
        "def cnn_2_1(input_shape):\n",
        "    return Sequential([\n",
        "        Input(input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 1),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "full_test(cnn_2_1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU4ESCxH4C9i",
        "outputId": "736464cd-312c-4c24-c5f4-9adf77386719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\me\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 - 8s - 583ms/step - accuracy: 0.2122 - loss: 1.6208 - val_accuracy: 0.2512 - val_loss: 1.5823\n",
            "Epoch 2/50\n",
            "14/14 - 7s - 470ms/step - accuracy: 0.3842 - loss: 1.4859 - val_accuracy: 0.5266 - val_loss: 1.2830\n",
            "Epoch 3/50\n",
            "14/14 - 7s - 467ms/step - accuracy: 0.4644 - loss: 1.2461 - val_accuracy: 0.6377 - val_loss: 1.0758\n",
            "Epoch 4/50\n",
            "14/14 - 7s - 473ms/step - accuracy: 0.5722 - loss: 1.0748 - val_accuracy: 0.4734 - val_loss: 1.1146\n",
            "Epoch 5/50\n",
            "14/14 - 7s - 482ms/step - accuracy: 0.5734 - loss: 1.0658 - val_accuracy: 0.7536 - val_loss: 0.8661\n",
            "Epoch 6/50\n",
            "14/14 - 7s - 464ms/step - accuracy: 0.6755 - loss: 0.8992 - val_accuracy: 0.7150 - val_loss: 0.8432\n",
            "Epoch 7/50\n",
            "14/14 - 7s - 492ms/step - accuracy: 0.6732 - loss: 0.8849 - val_accuracy: 0.7874 - val_loss: 0.7316\n",
            "Epoch 8/50\n",
            "14/14 - 7s - 488ms/step - accuracy: 0.7672 - loss: 0.7006 - val_accuracy: 0.7150 - val_loss: 0.7155\n",
            "Epoch 9/50\n",
            "14/14 - 7s - 484ms/step - accuracy: 0.7810 - loss: 0.6262 - val_accuracy: 0.7729 - val_loss: 0.6404\n",
            "Epoch 10/50\n",
            "14/14 - 7s - 473ms/step - accuracy: 0.8349 - loss: 0.5380 - val_accuracy: 0.7536 - val_loss: 0.6821\n",
            "Epoch 11/50\n",
            "14/14 - 6s - 460ms/step - accuracy: 0.8142 - loss: 0.5082 - val_accuracy: 0.7343 - val_loss: 0.7003\n",
            "Epoch 12/50\n",
            "14/14 - 7s - 466ms/step - accuracy: 0.8360 - loss: 0.4608 - val_accuracy: 0.7681 - val_loss: 0.6312\n",
            "Epoch 13/50\n",
            "14/14 - 7s - 465ms/step - accuracy: 0.8876 - loss: 0.3415 - val_accuracy: 0.7826 - val_loss: 0.6192\n",
            "Epoch 14/50\n",
            "14/14 - 7s - 472ms/step - accuracy: 0.9025 - loss: 0.3190 - val_accuracy: 0.7246 - val_loss: 0.6935\n",
            "Epoch 15/50\n",
            "14/14 - 6s - 464ms/step - accuracy: 0.8922 - loss: 0.3020 - val_accuracy: 0.7874 - val_loss: 0.6486\n",
            "Epoch 16/50\n",
            "14/14 - 7s - 467ms/step - accuracy: 0.9151 - loss: 0.2615 - val_accuracy: 0.7826 - val_loss: 0.5806\n",
            "Epoch 17/50\n",
            "14/14 - 7s - 469ms/step - accuracy: 0.9094 - loss: 0.2581 - val_accuracy: 0.8406 - val_loss: 0.4905\n",
            "Epoch 18/50\n",
            "14/14 - 7s - 474ms/step - accuracy: 0.9289 - loss: 0.2297 - val_accuracy: 0.7729 - val_loss: 0.6121\n",
            "Epoch 19/50\n",
            "14/14 - 7s - 465ms/step - accuracy: 0.9094 - loss: 0.2351 - val_accuracy: 0.8019 - val_loss: 0.5726\n",
            "Epoch 20/50\n",
            "14/14 - 7s - 466ms/step - accuracy: 0.9312 - loss: 0.2138 - val_accuracy: 0.7923 - val_loss: 0.6439\n",
            "Epoch 21/50\n",
            "14/14 - 9s - 631ms/step - accuracy: 0.9495 - loss: 0.1642 - val_accuracy: 0.8164 - val_loss: 0.5346\n",
            "Epoch 22/50\n",
            "14/14 - 7s - 487ms/step - accuracy: 0.9690 - loss: 0.1157 - val_accuracy: 0.7778 - val_loss: 0.6775\n",
            "Epoch 23/50\n",
            "14/14 - 7s - 477ms/step - accuracy: 0.9656 - loss: 0.1143 - val_accuracy: 0.7681 - val_loss: 0.6994\n",
            "Epoch 24/50\n",
            "14/14 - 7s - 477ms/step - accuracy: 0.9461 - loss: 0.1579 - val_accuracy: 0.8213 - val_loss: 0.5159\n",
            "Epoch 25/50\n",
            "14/14 - 7s - 468ms/step - accuracy: 0.9725 - loss: 0.0991 - val_accuracy: 0.7971 - val_loss: 0.5649\n",
            "Epoch 26/50\n",
            "14/14 - 7s - 476ms/step - accuracy: 0.9771 - loss: 0.0725 - val_accuracy: 0.8068 - val_loss: 0.6473\n",
            "Epoch 27/50\n",
            "14/14 - 7s - 483ms/step - accuracy: 0.9851 - loss: 0.0621 - val_accuracy: 0.8357 - val_loss: 0.6099\n",
            "\n",
            "Best epoch = 17\n",
            "  train_acc 0.9094 | train_loss 0.2581\n",
            "  val_acc   0.8406 | val_loss  0.4905\n",
            "\n",
            "Weighted F1 (val) : 0.8372\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.85      0.83      0.84        35\n",
            "          tekno       0.76      0.63      0.69        30\n",
            "        warzone       0.78      0.96      0.86        45\n",
            "     industrial       0.85      0.74      0.80        47\n",
            "non-techno-drop       0.92      0.96      0.94        50\n",
            "\n",
            "       accuracy                           0.84       207\n",
            "      macro avg       0.83      0.82      0.83       207\n",
            "   weighted avg       0.84      0.84      0.84       207\n",
            "\n",
            "Confusion matrix (val):\n",
            " [[29  2  0  2  2]\n",
            " [ 2 19  6  2  1]\n",
            " [ 0  1 43  1  0]\n",
            " [ 3  3  5 35  1]\n",
            " [ 0  0  1  1 48]]\n",
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.7952\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         bouncy       0.90      0.83      0.86        42\n",
            "          tekno       0.64      0.70      0.67        46\n",
            "        warzone       0.84      0.73      0.78        44\n",
            "     industrial       0.65      0.74      0.69        42\n",
            "non-techno-drop       0.96      0.94      0.95        52\n",
            "\n",
            "       accuracy                           0.79       226\n",
            "      macro avg       0.80      0.79      0.79       226\n",
            "   weighted avg       0.80      0.79      0.80       226\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[35  3  0  4  0]\n",
            " [ 3 32  2  9  0]\n",
            " [ 0  7 32  4  1]\n",
            " [ 0  8  2 31  1]\n",
            " [ 1  0  2  0 49]]\n"
          ]
        }
      ],
      "source": [
        "### rebuilding the best model so far for transfer-learning\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "final_model = Sequential([\n",
        "        Input(128,128,1),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(1, 2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "final_history = final_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=2          # prints: loss, acc, val_loss, val_acc each epoch\n",
        ")\n",
        "\n",
        "show_val_results(final_history, X_val, y_val, CLASSES)\n",
        "show_test_results(final_model,   X_test, y_test, CLASSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5yt8Lf4xdSu",
        "outputId": "72b27246-deb3-4a3c-c993-92cf6c827a4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as data/genres_transfer_model.h5\n"
          ]
        }
      ],
      "source": [
        "## saving the final model\n",
        "\n",
        "\n",
        "teliko_modelo = \"data/genres_transfer_model.h5\"\n",
        "final_model.save(teliko_modelo)\n",
        "print(f\"Model saved as {teliko_modelo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePlJDxXHxfac",
        "outputId": "ded38707-c99a-4965-c4f1-b16955855c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "  X_train: (699, 128, 128, 1), y_train: (699,)\n",
            "  X_val:   (150, 128, 128, 1),   y_val:   (150,)\n",
            "  X_test:  (150, 128, 128, 1),  y_test:  (150,)\n"
          ]
        }
      ],
      "source": [
        "### prep-split for transfer learning\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "DATA_DIR = \"data/transfer_set\"\n",
        "IMAGE_SIZE = (128, 128)\n",
        "SEED = 42\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# list of genre folders\n",
        "genres = sorted(os.listdir(DATA_DIR))\n",
        "genre_to_idx = {genre: idx for idx, genre in enumerate(genres)}\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for genre in genres:\n",
        "    genre_folder = os.path.join(DATA_DIR, genre)\n",
        "    for fname in os.listdir(genre_folder):\n",
        "        if fname.lower().endswith('.png'):\n",
        "            img_path = os.path.join(genre_folder, fname)\n",
        "\n",
        "            # grayscale AGAIN\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, IMAGE_SIZE)\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "            X.append(img)\n",
        "            y.append(genre_to_idx[genre])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# split into train 70%, temp 30% >> then split temp into val & test\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    random_state=SEED,\n",
        "    stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=SEED,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"  X_val:   {X_val.shape},   y_val:   {y_val.shape}\")\n",
        "print(f\"  X_test:  {X_test.shape},  y_test:  {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlLxmtAJxjzs",
        "outputId": "c0f7e1e5-967a-4b89-f715-802bcbf28dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 283ms/step - accuracy: 0.1041 - loss: 3.9832 - val_accuracy: 0.1000 - val_loss: 2.2988\n",
            "Epoch 2/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 282ms/step - accuracy: 0.0873 - loss: 2.3004 - val_accuracy: 0.1133 - val_loss: 2.2763\n",
            "Epoch 3/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 276ms/step - accuracy: 0.1209 - loss: 2.2686 - val_accuracy: 0.2600 - val_loss: 2.2279\n",
            "Epoch 4/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 279ms/step - accuracy: 0.2006 - loss: 2.1944 - val_accuracy: 0.2867 - val_loss: 2.0695\n",
            "Epoch 5/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 322ms/step - accuracy: 0.2351 - loss: 2.0869 - val_accuracy: 0.4533 - val_loss: 1.9402\n",
            "Epoch 6/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 303ms/step - accuracy: 0.2683 - loss: 1.9993 - val_accuracy: 0.4200 - val_loss: 1.8220\n",
            "Epoch 7/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 286ms/step - accuracy: 0.3129 - loss: 1.9066 - val_accuracy: 0.4800 - val_loss: 1.6875\n",
            "Epoch 8/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.3883 - loss: 1.7126 - val_accuracy: 0.5133 - val_loss: 1.5288\n",
            "Epoch 9/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 280ms/step - accuracy: 0.4769 - loss: 1.6046 - val_accuracy: 0.5133 - val_loss: 1.5380\n",
            "Epoch 10/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 324ms/step - accuracy: 0.4696 - loss: 1.5135 - val_accuracy: 0.4933 - val_loss: 1.3716\n",
            "Epoch 11/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 292ms/step - accuracy: 0.4694 - loss: 1.4736 - val_accuracy: 0.5533 - val_loss: 1.3336\n",
            "Epoch 12/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.5119 - loss: 1.3599 - val_accuracy: 0.5600 - val_loss: 1.3543\n",
            "Epoch 13/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 301ms/step - accuracy: 0.5370 - loss: 1.2976 - val_accuracy: 0.5467 - val_loss: 1.2861\n",
            "Epoch 14/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 290ms/step - accuracy: 0.5155 - loss: 1.3411 - val_accuracy: 0.5600 - val_loss: 1.2205\n",
            "Epoch 15/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.5991 - loss: 1.1765 - val_accuracy: 0.5867 - val_loss: 1.1773\n",
            "Epoch 16/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.6341 - loss: 1.0806 - val_accuracy: 0.5600 - val_loss: 1.1598\n",
            "Epoch 17/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.6147 - loss: 1.1060 - val_accuracy: 0.6000 - val_loss: 1.1044\n",
            "Epoch 18/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.6628 - loss: 0.9290 - val_accuracy: 0.6000 - val_loss: 1.1353\n",
            "Epoch 19/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 283ms/step - accuracy: 0.6566 - loss: 0.9577 - val_accuracy: 0.5533 - val_loss: 1.1645\n",
            "Epoch 20/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.7017 - loss: 0.8875 - val_accuracy: 0.6000 - val_loss: 1.1416\n",
            "Epoch 21/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 272ms/step - accuracy: 0.7049 - loss: 0.8414 - val_accuracy: 0.5867 - val_loss: 1.1015\n",
            "Epoch 22/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 280ms/step - accuracy: 0.7458 - loss: 0.7569 - val_accuracy: 0.5733 - val_loss: 1.1265\n",
            "Epoch 23/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 282ms/step - accuracy: 0.7777 - loss: 0.7000 - val_accuracy: 0.6067 - val_loss: 1.1663\n",
            "Epoch 24/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.7696 - loss: 0.6605 - val_accuracy: 0.5867 - val_loss: 1.1350\n",
            "Epoch 25/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - accuracy: 0.7751 - loss: 0.6274 - val_accuracy: 0.5667 - val_loss: 1.2477\n",
            "Epoch 26/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 284ms/step - accuracy: 0.8193 - loss: 0.5237 - val_accuracy: 0.6000 - val_loss: 1.1926\n",
            "Epoch 27/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.8452 - loss: 0.4730 - val_accuracy: 0.6333 - val_loss: 1.2516\n",
            "Epoch 28/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.8584 - loss: 0.4025 - val_accuracy: 0.5933 - val_loss: 1.3009\n",
            "Epoch 29/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.8559 - loss: 0.4124 - val_accuracy: 0.5933 - val_loss: 1.2952\n",
            "Epoch 30/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.8703 - loss: 0.3712 - val_accuracy: 0.6267 - val_loss: 1.4823\n",
            "Epoch 31/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.8469 - loss: 0.4079 - val_accuracy: 0.5933 - val_loss: 1.4318\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1604f2f19c0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### CNN for transfer learning\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# the original model\n",
        "base_model = load_model(teliko_modelo, compile=False)\n",
        "\n",
        "# the output of the penultimate layer\n",
        "x = base_model.layers[-2].output\n",
        "\n",
        "# new output layer for 10 genres\n",
        "new_output = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create new model\n",
        "transfer_model = Model(inputs=base_model.layers[0].input, outputs=new_output)\n",
        "\n",
        "# freezing earlier layers\n",
        "for layer in transfer_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "transfer_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# training on the 10-genre dataset\n",
        "transfer_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DxBkSP5qQe",
        "outputId": "edee19e8-f795-4e54-8efc-c95015421fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TEST-SET RESULTS ===\n",
            "Weighted F1 (test): 0.5755\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.41      0.47      0.44        15\n",
            "   classical       0.86      0.80      0.83        15\n",
            "     country       0.27      0.40      0.32        15\n",
            "       disco       0.62      0.53      0.57        15\n",
            "      hiphop       0.60      0.40      0.48        15\n",
            "        jazz       0.65      0.87      0.74        15\n",
            "       metal       0.85      0.73      0.79        15\n",
            "         pop       0.69      0.73      0.71        15\n",
            "      reggae       0.58      0.47      0.52        15\n",
            "        rock       0.38      0.33      0.36        15\n",
            "\n",
            "    accuracy                           0.57       150\n",
            "   macro avg       0.59      0.57      0.58       150\n",
            "weighted avg       0.59      0.57      0.58       150\n",
            "\n",
            "Confusion matrix (test):\n",
            " [[ 7  0  7  0  0  1  0  0  0  0]\n",
            " [ 0 12  1  0  0  1  0  1  0  0]\n",
            " [ 3  0  6  0  0  3  0  0  0  3]\n",
            " [ 0  1  1  8  1  0  0  1  1  2]\n",
            " [ 0  0  2  2  6  0  1  1  3  0]\n",
            " [ 2  0  0  0  0 13  0  0  0  0]\n",
            " [ 1  0  0  0  1  0 11  0  0  2]\n",
            " [ 0  0  1  2  0  0  0 11  0  1]\n",
            " [ 2  0  1  1  2  1  0  1  7  0]\n",
            " [ 2  1  3  0  0  1  1  1  1  5]]\n"
          ]
        }
      ],
      "source": [
        "genres = sorted(os.listdir(DATA_DIR))\n",
        "genre_to_idx = {g:i for i,g in enumerate(genres)}\n",
        "trans_class_names = genres\n",
        "\n",
        "show_test_results(transfer_model, X_test, y_test, trans_class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76pjqr-050fX"
      },
      "outputs": [],
      "source": [
        "### RESULTS for the test's f1-score after 5 consecutive runs for *tranfer learning*\n",
        "\n",
        "\n",
        "# run 1\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6343\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.67      0.67      0.67        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.42      0.73      0.54        15\n",
        "#        disco       0.67      0.53      0.59        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.63      0.80      0.71        15\n",
        "#        metal       0.80      0.80      0.80        15\n",
        "#          pop       0.69      0.60      0.64        15\n",
        "#       reggae       0.73      0.53      0.62        15\n",
        "#         rock       0.57      0.27      0.36        15\n",
        "\n",
        "#     accuracy                           0.64       150\n",
        "#    macro avg       0.66      0.64      0.63       150\n",
        "# weighted avg       0.66      0.64      0.63       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[10  0  3  0  0  2  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 0  1 11  0  0  2  0  0  0  1]\n",
        "#  [ 0  1  2  8  4  0  0  0  0  0]\n",
        "#  [ 0  0  0  1  9  0  1  2  2  0]\n",
        "#  [ 2  0  1  0  0 12  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 12  0  0  1]\n",
        "#  [ 0  0  1  3  1  0  0  9  0  1]\n",
        "#  [ 1  0  2  0  1  1  1  1  8  0]\n",
        "#  [ 1  1  5  0  0  2  1  0  1  4]]\n",
        "\n",
        "\n",
        "# run 2\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6247\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.59      0.67      0.62        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.44      0.53      0.48        15\n",
        "#        disco       0.40      0.53      0.46        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.85      0.73      0.79        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.64      0.60      0.62        15\n",
        "#       reggae       0.89      0.53      0.67        15\n",
        "#         rock       0.46      0.40      0.43        15\n",
        "\n",
        "#     accuracy                           0.62       150\n",
        "#    macro avg       0.64      0.62      0.62       150\n",
        "# weighted avg       0.64      0.62      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[10  0  3  2  0  0  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 0  1  8  0  0  2  0  0  0  4]\n",
        "#  [ 0  1  1  8  4  0  0  1  0  0]\n",
        "#  [ 0  0  0  3  9  0  1  2  0  0]\n",
        "#  [ 3  0  1  0  0 11  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 11  0  0  2]\n",
        "#  [ 0  0  1  4  0  0  0  9  0  1]\n",
        "#  [ 1  0  0  2  2  0  1  1  8  0]\n",
        "#  [ 2  1  3  1  0  0  1  0  1  6]]\n",
        "\n",
        "# run 3\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6327\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.52      0.80      0.63        15\n",
        "#    classical       0.81      0.87      0.84        15\n",
        "#      country       0.64      0.60      0.62        15\n",
        "#        disco       0.53      0.53      0.53        15\n",
        "#       hiphop       0.57      0.53      0.55        15\n",
        "#         jazz       0.62      0.87      0.72        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.67      0.53      0.59        15\n",
        "#       reggae       0.69      0.60      0.64        15\n",
        "#         rock       0.62      0.33      0.43        15\n",
        "\n",
        "#     accuracy                           0.64       150\n",
        "#    macro avg       0.65      0.64      0.63       150\n",
        "# weighted avg       0.65      0.64      0.63       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[12  0  0  0  0  3  0  0  0  0]\n",
        "#  [ 0 13  1  0  0  0  0  1  0  0]\n",
        "#  [ 1  1  9  0  0  3  0  0  0  1]\n",
        "#  [ 1  1  1  8  4  0  0  0  0  0]\n",
        "#  [ 0  0  0  2  8  0  1  2  2  0]\n",
        "#  [ 2  0  0  0  0 13  0  0  0  0]\n",
        "#  [ 2  0  0  0  1  0 11  0  0  1]\n",
        "#  [ 0  0  1  5  0  0  0  8  0  1]\n",
        "#  [ 2  0  0  0  1  1  1  1  9  0]\n",
        "#  [ 3  1  2  0  0  1  1  0  2  5]]\n",
        "\n",
        "# run 4\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6214\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.57      0.53      0.55        15\n",
        "#    classical       0.82      0.93      0.88        15\n",
        "#      country       0.45      0.60      0.51        15\n",
        "#        disco       0.54      0.47      0.50        15\n",
        "#       hiphop       0.56      0.60      0.58        15\n",
        "#         jazz       0.71      0.80      0.75        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.67      0.67      0.67        15\n",
        "#       reggae       0.53      0.60      0.56        15\n",
        "#         rock       0.71      0.33      0.45        15\n",
        "\n",
        "#     accuracy                           0.63       150\n",
        "#    macro avg       0.63      0.63      0.62       150\n",
        "# weighted avg       0.63      0.63      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[ 8  0  3  0  0  1  0  0  3  0]\n",
        "#  [ 0 14  0  0  0  0  0  1  0  0]\n",
        "#  [ 0  1  9  1  0  3  0  0  0  1]\n",
        "#  [ 0  1  2  7  3  0  0  1  1  0]\n",
        "#  [ 0  0  0  1  9  0  1  2  2  0]\n",
        "#  [ 3  0  0  0  0 12  0  0  0  0]\n",
        "#  [ 1  0  1  1  1  0 11  0  0  0]\n",
        "#  [ 0  0  1  2  1  0  0 10  0  1]\n",
        "#  [ 1  0  1  0  2  0  1  1  9  0]\n",
        "#  [ 1  1  3  1  0  1  1  0  2  5]]\n",
        "\n",
        "# run 5\n",
        "\n",
        "# === TEST-SET RESULTS ===\n",
        "# Weighted F1 (test): 0.6196\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#        blues       0.67      0.53      0.59        15\n",
        "#    classical       0.74      0.93      0.82        15\n",
        "#      country       0.64      0.47      0.54        15\n",
        "#        disco       0.44      0.47      0.45        15\n",
        "#       hiphop       0.53      0.60      0.56        15\n",
        "#         jazz       0.67      0.93      0.78        15\n",
        "#        metal       0.79      0.73      0.76        15\n",
        "#          pop       0.53      0.60      0.56        15\n",
        "#       reggae       0.75      0.60      0.67        15\n",
        "#         rock       0.55      0.40      0.46        15\n",
        "\n",
        "#     accuracy                           0.63       150\n",
        "#    macro avg       0.63      0.63      0.62       150\n",
        "# weighted avg       0.63      0.63      0.62       150\n",
        "\n",
        "# Confusion matrix (test):\n",
        "#  [[ 8  0  1  2  0  3  0  0  1  0]\n",
        "#  [ 0 14  0  0  0  0  0  1  0  0]\n",
        "#  [ 0  2  7  1  0  2  0  1  0  2]\n",
        "#  [ 0  1  1  7  4  0  0  2  0  0]\n",
        "#  [ 0  0  0  3  9  0  1  2  0  0]\n",
        "#  [ 1  0  0  0  0 14  0  0  0  0]\n",
        "#  [ 1  0  0  0  1  0 11  0  0  2]\n",
        "#  [ 0  0  1  3  1  0  0  9  0  1]\n",
        "#  [ 1  0  0  0  2  0  1  2  9  0]\n",
        "#  [ 1  2  1  0  0  2  1  0  2  6]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbiLzFejxpCq",
        "outputId": "df97ba45-bcca-45db-9712-f06fe1719153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.8541, Validation Accuracy: 0.5867\n",
            "Class distribution in Training Set:\n",
            "  Class 0: 70 samples, 10.01%\n",
            "  Class 1: 70 samples, 10.01%\n",
            "  Class 2: 70 samples, 10.01%\n",
            "  Class 3: 70 samples, 10.01%\n",
            "  Class 4: 70 samples, 10.01%\n",
            "  Class 5: 69 samples, 9.87%\n",
            "  Class 6: 70 samples, 10.01%\n",
            "  Class 7: 70 samples, 10.01%\n",
            "  Class 8: 70 samples, 10.01%\n",
            "  Class 9: 70 samples, 10.01%\n",
            "  Total samples: 699\n",
            "\n",
            "Class distribution in Validation Set:\n",
            "  Class 0: 15 samples, 10.00%\n",
            "  Class 1: 15 samples, 10.00%\n",
            "  Class 2: 15 samples, 10.00%\n",
            "  Class 3: 15 samples, 10.00%\n",
            "  Class 4: 15 samples, 10.00%\n",
            "  Class 5: 15 samples, 10.00%\n",
            "  Class 6: 15 samples, 10.00%\n",
            "  Class 7: 15 samples, 10.00%\n",
            "  Class 8: 15 samples, 10.00%\n",
            "  Class 9: 15 samples, 10.00%\n",
            "  Total samples: 150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#printing the accuracy and val accuracy of the final weights\n",
        "train_loss, train_acc = transfer_model.evaluate(X_train, y_train, verbose=0)\n",
        "val_loss, val_acc = transfer_model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# printing the per class distribution in x and y\n",
        "from collections import Counter\n",
        "def print_class_distribution(y, split_name):\n",
        "    counts = Counter(y)\n",
        "    total = len(y)\n",
        "    print(f\"Class distribution in {split_name}:\")\n",
        "\n",
        "    for cls, count in sorted(counts.items()):\n",
        "        print(f\"  Class {cls}: {count} samples, {count / total:.2%}\")\n",
        "    print(f\"  Total samples: {total}\\n\")\n",
        "print_class_distribution(y_train, 'Training Set')\n",
        "print_class_distribution(y_val, 'Validation Set')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}